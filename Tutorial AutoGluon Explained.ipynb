{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09d27c5b",
   "metadata": {
    "papermill": {
     "duration": 0.006241,
     "end_time": "2024-09-10T14:28:42.685393",
     "exception": false,
     "start_time": "2024-09-10T14:28:42.679152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tutorial - AutoGluon Explained\n",
    "![](https://auto.gluon.ai/stable/_static/autogluon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0d5b8",
   "metadata": {
    "papermill": {
     "duration": 0.005229,
     "end_time": "2024-09-10T14:28:42.696404",
     "exception": false,
     "start_time": "2024-09-10T14:28:42.691175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a1abfa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:28:42.709353Z",
     "iopub.status.busy": "2024-09-10T14:28:42.708495Z",
     "iopub.status.idle": "2024-09-10T14:29:56.170324Z",
     "shell.execute_reply": "2024-09-10T14:29:56.169100Z"
    },
    "papermill": {
     "duration": 73.470807,
     "end_time": "2024-09-10T14:29:56.172989",
     "exception": false,
     "start_time": "2024-09-10T14:28:42.702182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ray==2.10.0\r\n",
      "  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (8.1.7)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (3.15.1)\r\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (4.22.0)\r\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (1.0.8)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (21.3)\r\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (3.20.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (6.0.2)\r\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (1.3.1)\r\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (1.4.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (2.32.3)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0) (23.2.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0) (2023.12.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0) (0.35.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0) (0.18.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ray==2.10.0) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0) (2024.7.4)\r\n",
      "Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: ray\r\n",
      "  Attempting uninstall: ray\r\n",
      "    Found existing installation: ray 2.24.0\r\n",
      "    Uninstalling ray-2.24.0:\r\n",
      "      Successfully uninstalled ray-2.24.0\r\n",
      "Successfully installed ray-2.10.0\r\n",
      "Collecting autogluon.tabular\r\n",
      "  Downloading autogluon.tabular-1.1.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy<1.29,>=1.21 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular) (1.26.4)\r\n",
      "Collecting scipy<1.13,>=1.5.4 (from autogluon.tabular)\r\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pandas<2.3.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular) (2.2.2)\r\n",
      "Collecting scikit-learn<1.4.1,>=1.3.0 (from autogluon.tabular)\r\n",
      "  Downloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: networkx<4,>=3.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular) (3.3)\r\n",
      "Collecting autogluon.core==1.1.1 (from autogluon.tabular)\r\n",
      "  Downloading autogluon.core-1.1.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting autogluon.features==1.1.1 (from autogluon.tabular)\r\n",
      "  Downloading autogluon.features-1.1.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.tabular) (4.66.4)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.tabular) (2.32.3)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.tabular) (3.7.5)\r\n",
      "Requirement already satisfied: boto3<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.tabular) (1.26.100)\r\n",
      "Collecting autogluon.common==1.1.1 (from autogluon.core==1.1.1->autogluon.tabular)\r\n",
      "  Downloading autogluon.common-1.1.1-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: psutil<6,>=5.7.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.tabular) (5.9.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.tabular) (70.0.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2024.1)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular) (3.5.0)\r\n",
      "Collecting botocore<1.30.0,>=1.29.100 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular)\r\n",
      "  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular) (1.0.1)\r\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular) (0.6.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=2.0.0->autogluon.tabular) (1.16.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (1.2.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (4.53.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (21.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (3.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (2024.7.4)\r\n",
      "Downloading autogluon.tabular-1.1.1-py3-none-any.whl (312 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.core-1.1.1-py3-none-any.whl (234 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.8/234.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.features-1.1.1-py3-none-any.whl (63 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading autogluon.common-1.1.1-py3-none-any.whl (64 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: scipy, scikit-learn, botocore, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\r\n",
      "  Attempting uninstall: scipy\r\n",
      "    Found existing installation: scipy 1.14.0\r\n",
      "    Uninstalling scipy-1.14.0:\r\n",
      "      Successfully uninstalled scipy-1.14.0\r\n",
      "  Attempting uninstall: scikit-learn\r\n",
      "    Found existing installation: scikit-learn 1.2.2\r\n",
      "    Uninstalling scikit-learn-1.2.2:\r\n",
      "      Successfully uninstalled scikit-learn-1.2.2\r\n",
      "  Attempting uninstall: botocore\r\n",
      "    Found existing installation: botocore 1.34.131\r\n",
      "    Uninstalling botocore-1.34.131:\r\n",
      "      Successfully uninstalled botocore-1.34.131\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "aiobotocore 2.13.2 requires botocore<1.34.132,>=1.34.70, but you have botocore 1.29.165 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "bigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\r\n",
      "dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.8.2 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "pointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed autogluon.common-1.1.1 autogluon.core-1.1.1 autogluon.features-1.1.1 autogluon.tabular-1.1.1 botocore-1.29.165 scikit-learn-1.4.0 scipy-1.12.0\r\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (7.7.1)\r\n",
      "Collecting ipywidgets\r\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.21.0)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\r\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\r\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\r\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\r\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\r\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\r\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\r\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\r\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\r\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\r\n",
      "  Attempting uninstall: widgetsnbextension\r\n",
      "    Found existing installation: widgetsnbextension 3.6.8\r\n",
      "    Uninstalling widgetsnbextension-3.6.8:\r\n",
      "      Successfully uninstalled widgetsnbextension-3.6.8\r\n",
      "  Attempting uninstall: jupyterlab-widgets\r\n",
      "    Found existing installation: jupyterlab_widgets 3.0.11\r\n",
      "    Uninstalling jupyterlab_widgets-3.0.11:\r\n",
      "      Successfully uninstalled jupyterlab_widgets-3.0.11\r\n",
      "  Attempting uninstall: ipywidgets\r\n",
      "    Found existing installation: ipywidgets 7.7.1\r\n",
      "    Uninstalling ipywidgets-7.7.1:\r\n",
      "      Successfully uninstalled ipywidgets-7.7.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\r\n",
      "bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "bigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\r\n",
      "dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.8.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ray==2.10.0\n",
    "!pip install autogluon.tabular\n",
    "!pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ad309",
   "metadata": {
    "papermill": {
     "duration": 0.014196,
     "end_time": "2024-09-10T14:29:56.201716",
     "exception": false,
     "start_time": "2024-09-10T14:29:56.187520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc062b9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-10T14:29:56.231605Z",
     "iopub.status.busy": "2024-09-10T14:29:56.230958Z",
     "iopub.status.idle": "2024-09-10T14:29:58.743989Z",
     "shell.execute_reply": "2024-09-10T14:29:58.743197Z"
    },
    "papermill": {
     "duration": 2.530594,
     "end_time": "2024-09-10T14:29:58.746251",
     "exception": false,
     "start_time": "2024-09-10T14:29:56.215657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d7ed74",
   "metadata": {
    "papermill": {
     "duration": 0.013256,
     "end_time": "2024-09-10T14:29:58.772857",
     "exception": false,
     "start_time": "2024-09-10T14:29:58.759601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aab67cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:29:58.800604Z",
     "iopub.status.busy": "2024-09-10T14:29:58.800061Z",
     "iopub.status.idle": "2024-09-10T14:30:00.456861Z",
     "shell.execute_reply": "2024-09-10T14:30:00.455705Z"
    },
    "papermill": {
     "duration": 1.673701,
     "end_time": "2024-09-10T14:30:00.459580",
     "exception": false,
     "start_time": "2024-09-10T14:29:58.785879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/kaggle/input/playground-series-s4e9/train.csv').drop('id', axis=1)\n",
    "test_data = pd.read_csv('/kaggle/input/playground-series-s4e9/test.csv').drop('id', axis=1)\n",
    "submission = pd.read_csv('/kaggle/input/playground-series-s4e9/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acdf3db",
   "metadata": {
    "papermill": {
     "duration": 0.013399,
     "end_time": "2024-09-10T14:30:00.487071",
     "exception": false,
     "start_time": "2024-09-10T14:30:00.473672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# View Training and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c5249c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:30:00.514793Z",
     "iopub.status.busy": "2024-09-10T14:30:00.514185Z",
     "iopub.status.idle": "2024-09-10T14:30:00.533746Z",
     "shell.execute_reply": "2024-09-10T14:30:00.532740Z"
    },
    "papermill": {
     "duration": 0.035736,
     "end_time": "2024-09-10T14:30:00.535743",
     "exception": false,
     "start_time": "2024-09-10T14:30:00.500007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>model_year</th>\n",
       "      <th>milage</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>engine</th>\n",
       "      <th>transmission</th>\n",
       "      <th>ext_col</th>\n",
       "      <th>int_col</th>\n",
       "      <th>accident</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MINI</td>\n",
       "      <td>Cooper S Base</td>\n",
       "      <td>2007</td>\n",
       "      <td>213000</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>172.0HP 1.6L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>A/T</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>Gray</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lincoln</td>\n",
       "      <td>LS V8</td>\n",
       "      <td>2002</td>\n",
       "      <td>143250</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>252.0HP 3.9L 8 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>A/T</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Beige</td>\n",
       "      <td>At least 1 accident or damage reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Silverado 2500 LT</td>\n",
       "      <td>2002</td>\n",
       "      <td>136731</td>\n",
       "      <td>E85 Flex Fuel</td>\n",
       "      <td>320.0HP 5.3L 8 Cylinder Engine Flex Fuel Capab...</td>\n",
       "      <td>A/T</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Gray</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>13900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>G90 5.0 Ultimate</td>\n",
       "      <td>2017</td>\n",
       "      <td>19500</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>420.0HP 5.0L 8 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>Transmission w/Dual Shift Mode</td>\n",
       "      <td>Black</td>\n",
       "      <td>Black</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>45000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mercedes-Benz</td>\n",
       "      <td>Metris Base</td>\n",
       "      <td>2021</td>\n",
       "      <td>7388</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>208.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>7-Speed A/T</td>\n",
       "      <td>Black</td>\n",
       "      <td>Beige</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "      <td>97500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           brand              model  model_year  milage      fuel_type  \\\n",
       "0           MINI      Cooper S Base        2007  213000       Gasoline   \n",
       "1        Lincoln              LS V8        2002  143250       Gasoline   \n",
       "2      Chevrolet  Silverado 2500 LT        2002  136731  E85 Flex Fuel   \n",
       "3        Genesis   G90 5.0 Ultimate        2017   19500       Gasoline   \n",
       "4  Mercedes-Benz        Metris Base        2021    7388       Gasoline   \n",
       "\n",
       "                                              engine  \\\n",
       "0       172.0HP 1.6L 4 Cylinder Engine Gasoline Fuel   \n",
       "1       252.0HP 3.9L 8 Cylinder Engine Gasoline Fuel   \n",
       "2  320.0HP 5.3L 8 Cylinder Engine Flex Fuel Capab...   \n",
       "3       420.0HP 5.0L 8 Cylinder Engine Gasoline Fuel   \n",
       "4       208.0HP 2.0L 4 Cylinder Engine Gasoline Fuel   \n",
       "\n",
       "                     transmission ext_col int_col  \\\n",
       "0                             A/T  Yellow    Gray   \n",
       "1                             A/T  Silver   Beige   \n",
       "2                             A/T    Blue    Gray   \n",
       "3  Transmission w/Dual Shift Mode   Black   Black   \n",
       "4                     7-Speed A/T   Black   Beige   \n",
       "\n",
       "                                 accident clean_title  price  \n",
       "0                           None reported         Yes   4200  \n",
       "1  At least 1 accident or damage reported         Yes   4999  \n",
       "2                           None reported         Yes  13900  \n",
       "3                           None reported         Yes  45000  \n",
       "4                           None reported         Yes  97500  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91159089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:30:00.564144Z",
     "iopub.status.busy": "2024-09-10T14:30:00.563847Z",
     "iopub.status.idle": "2024-09-10T14:30:00.577957Z",
     "shell.execute_reply": "2024-09-10T14:30:00.577010Z"
    },
    "papermill": {
     "duration": 0.030374,
     "end_time": "2024-09-10T14:30:00.579820",
     "exception": false,
     "start_time": "2024-09-10T14:30:00.549446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>model_year</th>\n",
       "      <th>milage</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>engine</th>\n",
       "      <th>transmission</th>\n",
       "      <th>ext_col</th>\n",
       "      <th>int_col</th>\n",
       "      <th>accident</th>\n",
       "      <th>clean_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Land</td>\n",
       "      <td>Rover LR2 Base</td>\n",
       "      <td>2015</td>\n",
       "      <td>98000</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>6-Speed A/T</td>\n",
       "      <td>White</td>\n",
       "      <td>Beige</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Land</td>\n",
       "      <td>Rover Defender SE</td>\n",
       "      <td>2020</td>\n",
       "      <td>9142</td>\n",
       "      <td>Hybrid</td>\n",
       "      <td>395.0HP 3.0L Straight 6 Cylinder Engine Gasoli...</td>\n",
       "      <td>8-Speed A/T</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Black</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ford</td>\n",
       "      <td>Expedition Limited</td>\n",
       "      <td>2022</td>\n",
       "      <td>28121</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>3.5L V6 24V PDI DOHC Twin Turbo</td>\n",
       "      <td>10-Speed Automatic</td>\n",
       "      <td>White</td>\n",
       "      <td>Ebony</td>\n",
       "      <td>None reported</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Audi</td>\n",
       "      <td>A6 2.0T Sport</td>\n",
       "      <td>2016</td>\n",
       "      <td>61258</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>2.0 Liter TFSI</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>Silician Yellow</td>\n",
       "      <td>Black</td>\n",
       "      <td>None reported</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Audi</td>\n",
       "      <td>A6 2.0T Premium Plus</td>\n",
       "      <td>2018</td>\n",
       "      <td>59000</td>\n",
       "      <td>Gasoline</td>\n",
       "      <td>252.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n",
       "      <td>A/T</td>\n",
       "      <td>Gray</td>\n",
       "      <td>Black</td>\n",
       "      <td>None reported</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  brand                 model  model_year  milage fuel_type  \\\n",
       "0  Land        Rover LR2 Base        2015   98000  Gasoline   \n",
       "1  Land     Rover Defender SE        2020    9142    Hybrid   \n",
       "2  Ford    Expedition Limited        2022   28121  Gasoline   \n",
       "3  Audi         A6 2.0T Sport        2016   61258  Gasoline   \n",
       "4  Audi  A6 2.0T Premium Plus        2018   59000  Gasoline   \n",
       "\n",
       "                                              engine        transmission  \\\n",
       "0       240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel         6-Speed A/T   \n",
       "1  395.0HP 3.0L Straight 6 Cylinder Engine Gasoli...         8-Speed A/T   \n",
       "2                    3.5L V6 24V PDI DOHC Twin Turbo  10-Speed Automatic   \n",
       "3                                     2.0 Liter TFSI           Automatic   \n",
       "4       252.0HP 2.0L 4 Cylinder Engine Gasoline Fuel                 A/T   \n",
       "\n",
       "           ext_col int_col       accident clean_title  \n",
       "0            White   Beige  None reported         Yes  \n",
       "1           Silver   Black  None reported         Yes  \n",
       "2            White   Ebony  None reported         NaN  \n",
       "3  Silician Yellow   Black  None reported         NaN  \n",
       "4             Gray   Black  None reported         Yes  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9aebb0",
   "metadata": {
    "papermill": {
     "duration": 0.015095,
     "end_time": "2024-09-10T14:30:00.608693",
     "exception": false,
     "start_time": "2024-09-10T14:30:00.593598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AutoGluon Introduction\n",
    "\n",
    "AutoGluon is an open-source AutoML framework developed by Amazon that simplifies the process of building machine learning models for various tasks, including tabular data prediction, image classification, text analysis, and more. It automates the entire machine learning pipeline, from data preprocessing to model selection and hyperparameter tuning, making it accessible for users with minimal coding and machine learning expertise. AutoGluon supports both classification and regression problems and leverages powerful ensemble techniques to deliver high-quality models. It also allows users to specify resource constraints, like time limits and hardware availability (GPUs/CPUs), to optimize model training efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbef3d5",
   "metadata": {
    "papermill": {
     "duration": 0.014119,
     "end_time": "2024-09-10T14:30:00.636721",
     "exception": false,
     "start_time": "2024-09-10T14:30:00.622602",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AutoGluon Code with Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d472de14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:30:00.666082Z",
     "iopub.status.busy": "2024-09-10T14:30:00.665779Z",
     "iopub.status.idle": "2024-09-10T14:39:35.872844Z",
     "shell.execute_reply": "2024-09-10T14:39:35.871771Z"
    },
    "papermill": {
     "duration": 575.224196,
     "end_time": "2024-09-10T14:39:35.875000",
     "exception": false,
     "start_time": "2024-09-10T14:30:00.650804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240910_143000\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.14\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n",
      "CPU Count:          4\n",
      "Memory Avail:       30.15 GB / 31.36 GB (96.1%)\n",
      "Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 90s of the 360s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-09-10 14:30:03,870\tINFO worker.py:1752 -- Started a local Ray instance.\n",
      "\t\tContext path: \"AutogluonModels/ag-20240910_143000/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Beginning AutoGluon training ... Time limit = 85s\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon will save models to \"AutogluonModels/ag-20240910_143000/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Train Data Rows:    167584\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Train Data Columns: 11\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Label Column:       price\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tAvailable Memory:                    30422.15 MB\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTrain Data (Original)  Memory Usage: 102.42 MB (0.3% of available memory)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting TextSpecialFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t\tFitting BinnedFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting TextNgramFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t\tFitting CountVectorizer for text features: ['engine']\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t\tCountVectorizer fit with vocabulary size = 116\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', [])          : 2 | ['model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('object', [])       : 8 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('object', ['text']) : 1 | ['engine']\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('category', [])                    :  7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('category', ['text_as_category'])  :  1 | ['engine']\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', [])                         :  2 | ['model_year', 'milage']\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', ['bool'])                   :  1 | ['clean_title']\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', ['text_ngram'])             : 63 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t18.1s = Fit runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t11 features in original data used to generate 85 features in processed data.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTrain Data (Processed) Memory Usage: 26.53 MB (0.1% of available memory)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Data preprocessing and feature engineering runtime = 18.19s ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting 106 L1 models ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 44.75s of the 67.12s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.64%)\n",
      "\u001b[36m(_ray_fit pid=455)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=455)\u001b[0m [LightGBM] [Fatal] bin size 1669 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=455)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=497)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=497)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=497)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=539)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=539)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=539)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=581)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=581)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=581)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=623)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=623)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=623)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=665)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=665)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=665)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=707)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=707)\u001b[0m [LightGBM] [Fatal] bin size 1664 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=707)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=749)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=749)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=749)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t-73344.2279\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t51.21s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.48s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 67.15s of the 10.44s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t-73344.2279\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.01s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting 106 L2 models ...\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 10.39s of the 10.34s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.66%)\n",
      "\u001b[36m(_ray_fit pid=881)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=881)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=881)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=881)\u001b[0m \tRan out of time, early stopping on iteration 12. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=881)\u001b[0m \t[12]\tvalid_set's rmse: 74405.9\n",
      "\u001b[36m(_ray_fit pid=922)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=922)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=922)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=922)\u001b[0m \tRan out of time, early stopping on iteration 11. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=922)\u001b[0m \t[11]\tvalid_set's rmse: 66298.6\n",
      "\u001b[36m(_ray_fit pid=964)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=964)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=964)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=964)\u001b[0m \tRan out of time, early stopping on iteration 13. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=964)\u001b[0m \t[13]\tvalid_set's rmse: 62431.9\n",
      "\u001b[36m(_ray_fit pid=1006)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1006)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=1006)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=1006)\u001b[0m \tRan out of time, early stopping on iteration 12. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1006)\u001b[0m \t[12]\tvalid_set's rmse: 80971.1\n",
      "\u001b[36m(_ray_fit pid=1048)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1048)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=1048)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=1048)\u001b[0m \tRan out of time, early stopping on iteration 12. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1048)\u001b[0m \t[12]\tvalid_set's rmse: 75903.7\n",
      "\u001b[36m(_ray_fit pid=1090)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1090)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=1090)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=1090)\u001b[0m \tRan out of time, early stopping on iteration 10. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1090)\u001b[0m \t[10]\tvalid_set's rmse: 77665.8\n",
      "\u001b[36m(_ray_fit pid=1132)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1132)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=1132)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=1132)\u001b[0m \tRan out of time, early stopping on iteration 13. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1132)\u001b[0m \t[13]\tvalid_set's rmse: 77899.4\n",
      "\u001b[36m(_ray_fit pid=1174)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\u001b[36m(_ray_fit pid=1174)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n",
      "\u001b[36m(_ray_fit pid=1174)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=1174)\u001b[0m \tRan out of time, early stopping on iteration 12. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1174)\u001b[0m \t[12]\tvalid_set's rmse: 84474.3\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t-75318.8005\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t41.14s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.2s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 67.15s of the -33.67s of remaining time.\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t-73344.2279\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.11s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon training complete, total runtime = 119.18s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 43558.7 rows/s (20948 batch size)\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240910_143000/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=189)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout     score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    LightGBMXT_BAG_L1  -69893.921998 -73344.227854  root_mean_squared_error        0.573947       0.480238  51.213072                 0.573947                0.480238          51.213072            1       True          1\n",
      "1  WeightedEnsemble_L3  -69893.921998 -73344.227854  root_mean_squared_error        0.576177       0.482916  51.325703                 0.002230                0.002678           0.112631            3       True          4\n",
      "2  WeightedEnsemble_L2  -69893.921998 -73344.227854  root_mean_squared_error        0.576530       0.485649  51.220757                 0.002583                0.005411           0.007684            2       True          2\n",
      "3    LightGBMXT_BAG_L2  -72085.667100 -75318.800537  root_mean_squared_error        0.794673       0.681981  92.352014                 0.220725                0.201743          41.138942            2       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t128s\t = DyStack   runtime |\t232s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 232s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240910_143000\"\n",
      "Train Data Rows:    188533\n",
      "Train Data Columns: 11\n",
      "Label Column:       price\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    30562.62 MB\n",
      "\tTrain Data (Original)  Memory Usage: 115.28 MB (0.4% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['engine']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 116\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])          : 2 | ['model_year', 'milage']\n",
      "\t\t('object', [])       : 8 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t('object', ['text']) : 1 | ['engine']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :  7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "\t\t('category', ['text_as_category'])  :  1 | ['engine']\n",
      "\t\t('int', [])                         :  2 | ['model_year', 'milage']\n",
      "\t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "\t\t('int', ['bool'])                   :  1 | ['clean_title']\n",
      "\t\t('int', ['text_ngram'])             : 65 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "\t19.2s = Fit runtime\n",
      "\t11 features in original data used to generate 87 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 30.57 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 19.32s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 141.43s of the 212.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.69%)\n",
      "\t-72917.4814\t = Validation score   (-root_mean_squared_error)\n",
      "\t56.71s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 80.19s of the 150.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.69%)\n",
      "\t-73211.0061\t = Validation score   (-root_mean_squared_error)\n",
      "\t52.95s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 24.62s of the 95.37s of remaining time.\n",
      "\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 38.17s compared to 10s of available time.\n",
      "\tTime limit exceeded... Skipping RandomForestMSE_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 212.2s of the -214.03s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.765, 'LightGBM_BAG_L1': 0.235}\n",
      "\t-72886.2599\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 212.2s of the -214.61s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.765, 'LightGBM_BAG_L1': 0.235}\n",
      "\t-72886.2599\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 446.32s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 22258.8 rows/s (23567 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240910_143000\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label='price',             # Target column that needs to be predicted (dependent variable)\n",
    "    eval_metric='rmse',        # Evaluation metric (Root Mean Squared Error) used to judge the model’s performance\n",
    "    problem_type='regression'  # Specifying this is a regression problem\n",
    ").fit(\n",
    "    train_data,                  # The training dataset containing features and the target (price)\n",
    "    presets='best_quality',    # The preset configuration for optimal quality (though it may take more time)\n",
    "    time_limit=36*10,      # Time limit for training (10 hours = 3600 seconds/hour * 10 hours)\n",
    "    verbosity=2,               # Level of logging information (2 is medium verbosity)\n",
    "    excluded_model_types=['KNN'], # Exclude K-Nearest Neighbors models from training\n",
    "    ag_args_fit={\n",
    "        'num_gpus': 2,          # Use 2 GPUs if available for model training\n",
    "        'num_cpus': 4           # Use 4 CPUs for model training\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923bca9",
   "metadata": {
    "papermill": {
     "duration": 0.026875,
     "end_time": "2024-09-10T14:39:35.930809",
     "exception": false,
     "start_time": "2024-09-10T14:39:35.903934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* **label='price':** The column name 'price' is the target (dependent variable) to be predicted.\n",
    "* **eval_metric='rmse':** The Root Mean Squared Error (RMSE) is chosen as the evaluation metric, which is common for regression tasks.\n",
    "* **problem_type='regression':** Specifies that the task is a regression task (i.e., predicting continuous values).\n",
    "* **train_data:** This is the DataFrame containing the training data with both features and the target (price).\n",
    "* **presets='best_quality':** This preset prioritizes accuracy over training speed. It will try many models and techniques to ensure the highest possible quality.\n",
    "* **time_limit=3600*10:** Limits the model training process to a maximum of 10 hours.\n",
    "* **verbosity=2:** Specifies the verbosity level for logging. Higher values will show more details about the training process.\n",
    "* **excluded_model_types=['KNN']:** K-Nearest Neighbors (KNN) models are excluded from being considered during training.\n",
    "* **ag_args_fit:** This argument allows you to pass configuration options to the fitting process:\n",
    "* **num_gpus=2:** The model will utilize 2 GPUs for training if available, speeding up the process for certain algorithms.\n",
    "* **num_cpus=4:** The model will use 4 CPU cores during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c6378f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:39:35.987580Z",
     "iopub.status.busy": "2024-09-10T14:39:35.986257Z",
     "iopub.status.idle": "2024-09-10T14:39:36.714633Z",
     "shell.execute_reply": "2024-09-10T14:39:36.713446Z"
    },
    "papermill": {
     "duration": 0.759761,
     "end_time": "2024-09-10T14:39:36.716797",
     "exception": false,
     "start_time": "2024-09-10T14:39:35.957036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model     score_val              eval_metric  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  WeightedEnsemble_L3 -72886.259922  root_mean_squared_error       1.061062  109.763646                0.002635           0.110620            3       True          4\n",
      "1  WeightedEnsemble_L2 -72886.259922  root_mean_squared_error       1.061176  109.765974                0.002748           0.112947            2       True          3\n",
      "2    LightGBMXT_BAG_L1 -72917.481425  root_mean_squared_error       0.639680   56.707558                0.639680          56.707558            1       True          1\n",
      "3      LightGBM_BAG_L1 -73211.006130  root_mean_squared_error       0.418748   52.945468                0.418748          52.945468            1       True          2\n",
      "Number of models trained: 4\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_LGB', 'WeightedEnsembleModel'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])                    :  7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n",
      "('category', ['text_as_category'])  :  1 | ['engine']\n",
      "('int', [])                         :  2 | ['model_year', 'milage']\n",
      "('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n",
      "('int', ['bool'])                   :  1 | ['clean_title']\n",
      "('int', ['text_ngram'])             : 65 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20240910_143000SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    }
   ],
   "source": [
    "results = predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a25c3",
   "metadata": {
    "papermill": {
     "duration": 0.033176,
     "end_time": "2024-09-10T14:39:36.779797",
     "exception": false,
     "start_time": "2024-09-10T14:39:36.746621",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* **fit_summary():** After training is complete, this method outputs a summary of the models trained, their performance, and additional statistics. The results object will contain information such as the leaderboard of model performance, training times, and which model was selected as the best for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "864842aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:39:36.841047Z",
     "iopub.status.busy": "2024-09-10T14:39:36.840708Z",
     "iopub.status.idle": "2024-09-10T14:39:36.859614Z",
     "shell.execute_reply": "2024-09-10T14:39:36.858724Z"
    },
    "papermill": {
     "duration": 0.048794,
     "end_time": "2024-09-10T14:39:36.861749",
     "exception": false,
     "start_time": "2024-09-10T14:39:36.812955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>-72886.259922</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.061062</td>\n",
       "      <td>109.763646</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>0.110620</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-72886.259922</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.061176</td>\n",
       "      <td>109.765974</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.112947</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBMXT_BAG_L1</td>\n",
       "      <td>-72917.481425</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.639680</td>\n",
       "      <td>56.707558</td>\n",
       "      <td>0.639680</td>\n",
       "      <td>56.707558</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>-73211.006130</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.418748</td>\n",
       "      <td>52.945468</td>\n",
       "      <td>0.418748</td>\n",
       "      <td>52.945468</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model     score_val              eval_metric  pred_time_val  \\\n",
       "0  WeightedEnsemble_L3 -72886.259922  root_mean_squared_error       1.061062   \n",
       "1  WeightedEnsemble_L2 -72886.259922  root_mean_squared_error       1.061176   \n",
       "2    LightGBMXT_BAG_L1 -72917.481425  root_mean_squared_error       0.639680   \n",
       "3      LightGBM_BAG_L1 -73211.006130  root_mean_squared_error       0.418748   \n",
       "\n",
       "     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n",
       "0  109.763646                0.002635           0.110620            3   \n",
       "1  109.765974                0.002748           0.112947            2   \n",
       "2   56.707558                0.639680          56.707558            1   \n",
       "3   52.945468                0.418748          52.945468            1   \n",
       "\n",
       "   can_infer  fit_order  \n",
       "0       True          4  \n",
       "1       True          3  \n",
       "2       True          1  \n",
       "3       True          2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c2686b",
   "metadata": {
    "papermill": {
     "duration": 0.026103,
     "end_time": "2024-09-10T14:39:36.916811",
     "exception": false,
     "start_time": "2024-09-10T14:39:36.890708",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**What the Leaderboard Shows:**\n",
    "* **Model:** The name of the model that was trained. This can include various types of models such as Random Forest, Gradient Boosting, Neural Networks, etc.\n",
    "* **Time Training:** The time taken to train the model.\n",
    "* **Time Prediction:** The time taken to make predictions with the model.\n",
    "* **Score Validation:** The score (e.g., RMSE) on the validation set, indicating how well the model performs on data it hasn’t seen during training.\n",
    "* **Fit Order:** The order in which the models were trained.\n",
    "\n",
    "**Interpreting the Table:**\n",
    "* **WeightedEnsemble_L2:** This is an ensemble model that combines predictions from multiple other models (e.g., LightGBM, CatBoost). It is ranked first due to its lowest RMSE on the validation set (Score_Validation).\n",
    "* **LightGBM_BAG_L1:** A LightGBM model that was also considered. It shows slightly worse performance than the ensemble but may have taken less time to train (Training_Time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4bad7b",
   "metadata": {
    "papermill": {
     "duration": 0.02604,
     "end_time": "2024-09-10T14:39:36.971181",
     "exception": false,
     "start_time": "2024-09-10T14:39:36.945141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1371946d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:39:37.025246Z",
     "iopub.status.busy": "2024-09-10T14:39:37.024930Z",
     "iopub.status.idle": "2024-09-10T14:39:50.564993Z",
     "shell.execute_reply": "2024-09-10T14:39:50.563945Z"
    },
    "papermill": {
     "duration": 13.570428,
     "end_time": "2024-09-10T14:39:50.568005",
     "exception": false,
     "start_time": "2024-09-10T14:39:36.997577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred = predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f788b4",
   "metadata": {
    "papermill": {
     "duration": 0.035771,
     "end_time": "2024-09-10T14:39:50.641608",
     "exception": false,
     "start_time": "2024-09-10T14:39:50.605837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d78731f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:39:50.716182Z",
     "iopub.status.busy": "2024-09-10T14:39:50.715692Z",
     "iopub.status.idle": "2024-09-10T14:39:50.990936Z",
     "shell.execute_reply": "2024-09-10T14:39:50.989917Z"
    },
    "papermill": {
     "duration": 0.315156,
     "end_time": "2024-09-10T14:39:50.993190",
     "exception": false,
     "start_time": "2024-09-10T14:39:50.678034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission['price'] = test_pred\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9057646,
     "sourceId": 76728,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 676.282662,
   "end_time": "2024-09-10T14:39:56.140614",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-10T14:28:39.857952",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
