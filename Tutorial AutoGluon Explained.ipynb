{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/taimour/s4e9-tutorial-autogluon-explained?scriptVersionId=196086119\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"93782acf","metadata":{"papermill":{"duration":0.005944,"end_time":"2024-09-10T14:55:19.177816","exception":false,"start_time":"2024-09-10T14:55:19.171872","status":"completed"},"tags":[]},"source":["# Tutorial - AutoGluon Explained\n","![](https://auto.gluon.ai/stable/_static/autogluon.png)"]},{"cell_type":"markdown","id":"c71f5ca3","metadata":{"papermill":{"duration":0.00505,"end_time":"2024-09-10T14:55:19.188328","exception":false,"start_time":"2024-09-10T14:55:19.183278","status":"completed"},"tags":[]},"source":["# Installation"]},{"cell_type":"code","execution_count":1,"id":"00355532","metadata":{"execution":{"iopub.execute_input":"2024-09-10T14:55:19.200695Z","iopub.status.busy":"2024-09-10T14:55:19.200099Z","iopub.status.idle":"2024-09-10T14:56:30.893868Z","shell.execute_reply":"2024-09-10T14:56:30.89272Z"},"papermill":{"duration":71.702379,"end_time":"2024-09-10T14:56:30.896269","exception":false,"start_time":"2024-09-10T14:55:19.19389","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ray==2.10.0\r\n","  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\r\n","Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (8.1.7)\r\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (3.15.1)\r\n","Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (4.22.0)\r\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (1.0.8)\r\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (21.3)\r\n","Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (3.20.3)\r\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (6.0.2)\r\n","Requirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (1.3.1)\r\n","Requirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (1.4.1)\r\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from ray==2.10.0) (2.32.3)\r\n","Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0) (23.2.0)\r\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0) (2023.12.1)\r\n","Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0) (0.35.1)\r\n","Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.10.0) (0.18.1)\r\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ray==2.10.0) (3.1.2)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0) (3.3.2)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0) (3.7)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0) (1.26.18)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.10.0) (2024.7.4)\r\n","Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: ray\r\n","  Attempting uninstall: ray\r\n","    Found existing installation: ray 2.24.0\r\n","    Uninstalling ray-2.24.0:\r\n","      Successfully uninstalled ray-2.24.0\r\n","Successfully installed ray-2.10.0\r\n","Collecting autogluon.tabular\r\n","  Downloading autogluon.tabular-1.1.1-py3-none-any.whl.metadata (13 kB)\r\n","Requirement already satisfied: numpy<1.29,>=1.21 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular) (1.26.4)\r\n","Collecting scipy<1.13,>=1.5.4 (from autogluon.tabular)\r\n","  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hRequirement already satisfied: pandas<2.3.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular) (2.2.2)\r\n","Collecting scikit-learn<1.4.1,>=1.3.0 (from autogluon.tabular)\r\n","  Downloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n","Requirement already satisfied: networkx<4,>=3.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular) (3.3)\r\n","Collecting autogluon.core==1.1.1 (from autogluon.tabular)\r\n","  Downloading autogluon.core-1.1.1-py3-none-any.whl.metadata (11 kB)\r\n","Collecting autogluon.features==1.1.1 (from autogluon.tabular)\r\n","  Downloading autogluon.features-1.1.1-py3-none-any.whl.metadata (11 kB)\r\n","Requirement already satisfied: tqdm<5,>=4.38 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.tabular) (4.66.4)\r\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.tabular) (2.32.3)\r\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.tabular) (3.7.5)\r\n","Requirement already satisfied: boto3<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.tabular) (1.26.100)\r\n","Collecting autogluon.common==1.1.1 (from autogluon.core==1.1.1->autogluon.tabular)\r\n","  Downloading autogluon.common-1.1.1-py3-none-any.whl.metadata (11 kB)\r\n","Requirement already satisfied: psutil<6,>=5.7.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.tabular) (5.9.3)\r\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.tabular) (70.0.0)\r\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2.9.0.post0)\r\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2024.1)\r\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2024.1)\r\n","Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular) (1.4.2)\r\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular) (3.5.0)\r\n","Collecting botocore<1.30.0,>=1.29.100 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular)\r\n","  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\r\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular) (1.0.1)\r\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular) (0.6.2)\r\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=2.0.0->autogluon.tabular) (1.16.0)\r\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (1.2.1)\r\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (0.12.1)\r\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (4.53.0)\r\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (1.4.5)\r\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (21.3)\r\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (9.5.0)\r\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (3.1.2)\r\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (3.3.2)\r\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (3.7)\r\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (1.26.18)\r\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (2024.7.4)\r\n","Downloading autogluon.tabular-1.1.1-py3-none-any.whl (312 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading autogluon.core-1.1.1-py3-none-any.whl (234 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.8/234.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading autogluon.features-1.1.1-py3-none-any.whl (63 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading autogluon.common-1.1.1-py3-none-any.whl (64 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: scipy, scikit-learn, botocore, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\r\n","  Attempting uninstall: scipy\r\n","    Found existing installation: scipy 1.14.0\r\n","    Uninstalling scipy-1.14.0:\r\n","      Successfully uninstalled scipy-1.14.0\r\n","  Attempting uninstall: scikit-learn\r\n","    Found existing installation: scikit-learn 1.2.2\r\n","    Uninstalling scikit-learn-1.2.2:\r\n","      Successfully uninstalled scikit-learn-1.2.2\r\n","  Attempting uninstall: botocore\r\n","    Found existing installation: botocore 1.34.131\r\n","    Uninstalling botocore-1.34.131:\r\n","      Successfully uninstalled botocore-1.34.131\r\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","cuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n","aiobotocore 2.13.2 requires botocore<1.34.132,>=1.34.70, but you have botocore 1.29.165 which is incompatible.\r\n","bigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\r\n","bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n","bigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\r\n","dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.8.2 which is incompatible.\r\n","libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n","libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n","momepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n","pointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n","spaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n","spopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n","tsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0mSuccessfully installed autogluon.common-1.1.1 autogluon.core-1.1.1 autogluon.features-1.1.1 autogluon.tabular-1.1.1 botocore-1.29.165 scikit-learn-1.4.0 scipy-1.12.0\r\n","Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (7.7.1)\r\n","Collecting ipywidgets\r\n","  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\r\n","Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\r\n","Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.21.0)\r\n","Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\r\n","Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\r\n","  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\r\n","Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\r\n","  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\r\n","Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\r\n","Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\r\n","Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\r\n","Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\r\n","Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\r\n","Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\r\n","Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\r\n","Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\r\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\r\n","Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n","Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\r\n","Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\r\n","Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\r\n","Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\r\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\r\n","Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\r\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n","\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\r\n","  Attempting uninstall: widgetsnbextension\r\n","    Found existing installation: widgetsnbextension 3.6.8\r\n","    Uninstalling widgetsnbextension-3.6.8:\r\n","      Successfully uninstalled widgetsnbextension-3.6.8\r\n","  Attempting uninstall: jupyterlab-widgets\r\n","    Found existing installation: jupyterlab_widgets 3.0.11\r\n","    Uninstalling jupyterlab_widgets-3.0.11:\r\n","      Successfully uninstalled jupyterlab_widgets-3.0.11\r\n","  Attempting uninstall: ipywidgets\r\n","    Found existing installation: ipywidgets 7.7.1\r\n","    Uninstalling ipywidgets-7.7.1:\r\n","      Successfully uninstalled ipywidgets-7.7.1\r\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n","bigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\r\n","bigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n","bigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\r\n","dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.8.2 which is incompatible.\u001b[0m\u001b[31m\r\n","\u001b[0mSuccessfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\r\n"]}],"source":["!pip install ray==2.10.0\n","!pip install autogluon.tabular\n","!pip install -U ipywidgets"]},{"cell_type":"markdown","id":"4ba40b5c","metadata":{"papermill":{"duration":0.012587,"end_time":"2024-09-10T14:56:30.921506","exception":false,"start_time":"2024-09-10T14:56:30.908919","status":"completed"},"tags":[]},"source":["# Import"]},{"cell_type":"code","execution_count":2,"id":"413b3153","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-10T14:56:30.948395Z","iopub.status.busy":"2024-09-10T14:56:30.948012Z","iopub.status.idle":"2024-09-10T14:56:33.500763Z","shell.execute_reply":"2024-09-10T14:56:33.499772Z"},"papermill":{"duration":2.569014,"end_time":"2024-09-10T14:56:33.503151","exception":false,"start_time":"2024-09-10T14:56:30.934137","status":"completed"},"tags":[]},"outputs":[],"source":["import pandas as pd\n","from autogluon.tabular import TabularDataset, TabularPredictor"]},{"cell_type":"markdown","id":"03e15288","metadata":{"papermill":{"duration":0.012049,"end_time":"2024-09-10T14:56:33.527733","exception":false,"start_time":"2024-09-10T14:56:33.515684","status":"completed"},"tags":[]},"source":["# Read Data"]},{"cell_type":"code","execution_count":3,"id":"a3149328","metadata":{"execution":{"iopub.execute_input":"2024-09-10T14:56:33.553617Z","iopub.status.busy":"2024-09-10T14:56:33.553113Z","iopub.status.idle":"2024-09-10T14:56:34.926443Z","shell.execute_reply":"2024-09-10T14:56:34.925598Z"},"papermill":{"duration":1.388617,"end_time":"2024-09-10T14:56:34.928691","exception":false,"start_time":"2024-09-10T14:56:33.540074","status":"completed"},"tags":[]},"outputs":[],"source":["train_data = pd.read_csv('/kaggle/input/playground-series-s4e9/train.csv').drop('id', axis=1)\n","test_data = pd.read_csv('/kaggle/input/playground-series-s4e9/test.csv').drop('id', axis=1)\n","submission = pd.read_csv('/kaggle/input/playground-series-s4e9/sample_submission.csv')"]},{"cell_type":"markdown","id":"e688d959","metadata":{"papermill":{"duration":0.011861,"end_time":"2024-09-10T14:56:34.953196","exception":false,"start_time":"2024-09-10T14:56:34.941335","status":"completed"},"tags":[]},"source":["# View Training and Test data"]},{"cell_type":"code","execution_count":4,"id":"4955cf44","metadata":{"execution":{"iopub.execute_input":"2024-09-10T14:56:34.978651Z","iopub.status.busy":"2024-09-10T14:56:34.978327Z","iopub.status.idle":"2024-09-10T14:56:34.996495Z","shell.execute_reply":"2024-09-10T14:56:34.995566Z"},"papermill":{"duration":0.033534,"end_time":"2024-09-10T14:56:34.998782","exception":false,"start_time":"2024-09-10T14:56:34.965248","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>brand</th>\n","      <th>model</th>\n","      <th>model_year</th>\n","      <th>milage</th>\n","      <th>fuel_type</th>\n","      <th>engine</th>\n","      <th>transmission</th>\n","      <th>ext_col</th>\n","      <th>int_col</th>\n","      <th>accident</th>\n","      <th>clean_title</th>\n","      <th>price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>MINI</td>\n","      <td>Cooper S Base</td>\n","      <td>2007</td>\n","      <td>213000</td>\n","      <td>Gasoline</td>\n","      <td>172.0HP 1.6L 4 Cylinder Engine Gasoline Fuel</td>\n","      <td>A/T</td>\n","      <td>Yellow</td>\n","      <td>Gray</td>\n","      <td>None reported</td>\n","      <td>Yes</td>\n","      <td>4200</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lincoln</td>\n","      <td>LS V8</td>\n","      <td>2002</td>\n","      <td>143250</td>\n","      <td>Gasoline</td>\n","      <td>252.0HP 3.9L 8 Cylinder Engine Gasoline Fuel</td>\n","      <td>A/T</td>\n","      <td>Silver</td>\n","      <td>Beige</td>\n","      <td>At least 1 accident or damage reported</td>\n","      <td>Yes</td>\n","      <td>4999</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Chevrolet</td>\n","      <td>Silverado 2500 LT</td>\n","      <td>2002</td>\n","      <td>136731</td>\n","      <td>E85 Flex Fuel</td>\n","      <td>320.0HP 5.3L 8 Cylinder Engine Flex Fuel Capab...</td>\n","      <td>A/T</td>\n","      <td>Blue</td>\n","      <td>Gray</td>\n","      <td>None reported</td>\n","      <td>Yes</td>\n","      <td>13900</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Genesis</td>\n","      <td>G90 5.0 Ultimate</td>\n","      <td>2017</td>\n","      <td>19500</td>\n","      <td>Gasoline</td>\n","      <td>420.0HP 5.0L 8 Cylinder Engine Gasoline Fuel</td>\n","      <td>Transmission w/Dual Shift Mode</td>\n","      <td>Black</td>\n","      <td>Black</td>\n","      <td>None reported</td>\n","      <td>Yes</td>\n","      <td>45000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Mercedes-Benz</td>\n","      <td>Metris Base</td>\n","      <td>2021</td>\n","      <td>7388</td>\n","      <td>Gasoline</td>\n","      <td>208.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n","      <td>7-Speed A/T</td>\n","      <td>Black</td>\n","      <td>Beige</td>\n","      <td>None reported</td>\n","      <td>Yes</td>\n","      <td>97500</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           brand              model  model_year  milage      fuel_type  \\\n","0           MINI      Cooper S Base        2007  213000       Gasoline   \n","1        Lincoln              LS V8        2002  143250       Gasoline   \n","2      Chevrolet  Silverado 2500 LT        2002  136731  E85 Flex Fuel   \n","3        Genesis   G90 5.0 Ultimate        2017   19500       Gasoline   \n","4  Mercedes-Benz        Metris Base        2021    7388       Gasoline   \n","\n","                                              engine  \\\n","0       172.0HP 1.6L 4 Cylinder Engine Gasoline Fuel   \n","1       252.0HP 3.9L 8 Cylinder Engine Gasoline Fuel   \n","2  320.0HP 5.3L 8 Cylinder Engine Flex Fuel Capab...   \n","3       420.0HP 5.0L 8 Cylinder Engine Gasoline Fuel   \n","4       208.0HP 2.0L 4 Cylinder Engine Gasoline Fuel   \n","\n","                     transmission ext_col int_col  \\\n","0                             A/T  Yellow    Gray   \n","1                             A/T  Silver   Beige   \n","2                             A/T    Blue    Gray   \n","3  Transmission w/Dual Shift Mode   Black   Black   \n","4                     7-Speed A/T   Black   Beige   \n","\n","                                 accident clean_title  price  \n","0                           None reported         Yes   4200  \n","1  At least 1 accident or damage reported         Yes   4999  \n","2                           None reported         Yes  13900  \n","3                           None reported         Yes  45000  \n","4                           None reported         Yes  97500  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_data.head()"]},{"cell_type":"code","execution_count":5,"id":"3bde6af5","metadata":{"execution":{"iopub.execute_input":"2024-09-10T14:56:35.026234Z","iopub.status.busy":"2024-09-10T14:56:35.025941Z","iopub.status.idle":"2024-09-10T14:56:35.039976Z","shell.execute_reply":"2024-09-10T14:56:35.039112Z"},"papermill":{"duration":0.029447,"end_time":"2024-09-10T14:56:35.042052","exception":false,"start_time":"2024-09-10T14:56:35.012605","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>brand</th>\n","      <th>model</th>\n","      <th>model_year</th>\n","      <th>milage</th>\n","      <th>fuel_type</th>\n","      <th>engine</th>\n","      <th>transmission</th>\n","      <th>ext_col</th>\n","      <th>int_col</th>\n","      <th>accident</th>\n","      <th>clean_title</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Land</td>\n","      <td>Rover LR2 Base</td>\n","      <td>2015</td>\n","      <td>98000</td>\n","      <td>Gasoline</td>\n","      <td>240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n","      <td>6-Speed A/T</td>\n","      <td>White</td>\n","      <td>Beige</td>\n","      <td>None reported</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Land</td>\n","      <td>Rover Defender SE</td>\n","      <td>2020</td>\n","      <td>9142</td>\n","      <td>Hybrid</td>\n","      <td>395.0HP 3.0L Straight 6 Cylinder Engine Gasoli...</td>\n","      <td>8-Speed A/T</td>\n","      <td>Silver</td>\n","      <td>Black</td>\n","      <td>None reported</td>\n","      <td>Yes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ford</td>\n","      <td>Expedition Limited</td>\n","      <td>2022</td>\n","      <td>28121</td>\n","      <td>Gasoline</td>\n","      <td>3.5L V6 24V PDI DOHC Twin Turbo</td>\n","      <td>10-Speed Automatic</td>\n","      <td>White</td>\n","      <td>Ebony</td>\n","      <td>None reported</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Audi</td>\n","      <td>A6 2.0T Sport</td>\n","      <td>2016</td>\n","      <td>61258</td>\n","      <td>Gasoline</td>\n","      <td>2.0 Liter TFSI</td>\n","      <td>Automatic</td>\n","      <td>Silician Yellow</td>\n","      <td>Black</td>\n","      <td>None reported</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Audi</td>\n","      <td>A6 2.0T Premium Plus</td>\n","      <td>2018</td>\n","      <td>59000</td>\n","      <td>Gasoline</td>\n","      <td>252.0HP 2.0L 4 Cylinder Engine Gasoline Fuel</td>\n","      <td>A/T</td>\n","      <td>Gray</td>\n","      <td>Black</td>\n","      <td>None reported</td>\n","      <td>Yes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  brand                 model  model_year  milage fuel_type  \\\n","0  Land        Rover LR2 Base        2015   98000  Gasoline   \n","1  Land     Rover Defender SE        2020    9142    Hybrid   \n","2  Ford    Expedition Limited        2022   28121  Gasoline   \n","3  Audi         A6 2.0T Sport        2016   61258  Gasoline   \n","4  Audi  A6 2.0T Premium Plus        2018   59000  Gasoline   \n","\n","                                              engine        transmission  \\\n","0       240.0HP 2.0L 4 Cylinder Engine Gasoline Fuel         6-Speed A/T   \n","1  395.0HP 3.0L Straight 6 Cylinder Engine Gasoli...         8-Speed A/T   \n","2                    3.5L V6 24V PDI DOHC Twin Turbo  10-Speed Automatic   \n","3                                     2.0 Liter TFSI           Automatic   \n","4       252.0HP 2.0L 4 Cylinder Engine Gasoline Fuel                 A/T   \n","\n","           ext_col int_col       accident clean_title  \n","0            White   Beige  None reported         Yes  \n","1           Silver   Black  None reported         Yes  \n","2            White   Ebony  None reported         NaN  \n","3  Silician Yellow   Black  None reported         NaN  \n","4             Gray   Black  None reported         Yes  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["test_data.head()"]},{"cell_type":"markdown","id":"129106e1","metadata":{"papermill":{"duration":0.013665,"end_time":"2024-09-10T14:56:35.069041","exception":false,"start_time":"2024-09-10T14:56:35.055376","status":"completed"},"tags":[]},"source":["# AutoGluon Introduction\n","\n","AutoGluon is an open-source AutoML framework developed by Amazon that simplifies the process of building machine learning models for various tasks, including tabular data prediction, image classification, text analysis, and more. It automates the entire machine learning pipeline, from data preprocessing to model selection and hyperparameter tuning, making it accessible for users with minimal coding and machine learning expertise. AutoGluon supports both classification and regression problems and leverages powerful ensemble techniques to deliver high-quality models. It also allows users to specify resource constraints, like time limits and hardware availability (GPUs/CPUs), to optimize model training efficiency."]},{"cell_type":"markdown","id":"0911b845","metadata":{"papermill":{"duration":0.012432,"end_time":"2024-09-10T14:56:35.094608","exception":false,"start_time":"2024-09-10T14:56:35.082176","status":"completed"},"tags":[]},"source":["# AutoGluon Code with Explanation"]},{"cell_type":"code","execution_count":6,"id":"009802ab","metadata":{"execution":{"iopub.execute_input":"2024-09-10T14:56:35.121027Z","iopub.status.busy":"2024-09-10T14:56:35.120758Z","iopub.status.idle":"2024-09-10T17:57:13.592808Z","shell.execute_reply":"2024-09-10T17:57:13.591847Z"},"papermill":{"duration":10838.487796,"end_time":"2024-09-10T17:57:13.594871","exception":false,"start_time":"2024-09-10T14:56:35.107075","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["No path specified. Models will be saved in: \"AutogluonModels/ag-20240910_145635\"\n","Verbosity: 2 (Standard Logging)\n","=================== System Info ===================\n","AutoGluon Version:  1.1.1\n","Python Version:     3.10.14\n","Operating System:   Linux\n","Platform Machine:   x86_64\n","Platform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\n","CPU Count:          4\n","Memory Avail:       30.16 GB / 31.36 GB (96.2%)\n","Disk Space Avail:   19.50 GB / 19.52 GB (99.9%)\n","===================================================\n","Presets specified: ['best_quality']\n","Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n","Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n","DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n","\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n","\tRunning DyStack for up to 2700s of the 10800s of remaining time (25%).\n","\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n","2024-09-10 14:56:38,530\tINFO worker.py:1752 -- Started a local Ray instance.\n","\t\tContext path: \"AutogluonModels/ag-20240910_145635/ds_sub_fit/sub_fit_ho\"\n","\u001b[36m(_dystack pid=189)\u001b[0m Running DyStack sub-fit ...\n","\u001b[36m(_dystack pid=189)\u001b[0m Beginning AutoGluon training ... Time limit = 2695s\n","\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon will save models to \"AutogluonModels/ag-20240910_145635/ds_sub_fit/sub_fit_ho\"\n","\u001b[36m(_dystack pid=189)\u001b[0m Train Data Rows:    167584\n","\u001b[36m(_dystack pid=189)\u001b[0m Train Data Columns: 11\n","\u001b[36m(_dystack pid=189)\u001b[0m Label Column:       price\n","\u001b[36m(_dystack pid=189)\u001b[0m Problem Type:       regression\n","\u001b[36m(_dystack pid=189)\u001b[0m Preprocessing data ...\n","\u001b[36m(_dystack pid=189)\u001b[0m Using Feature Generators to preprocess the data ...\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n","\u001b[36m(_dystack pid=189)\u001b[0m \tAvailable Memory:                    30469.23 MB\n","\u001b[36m(_dystack pid=189)\u001b[0m \tTrain Data (Original)  Memory Usage: 102.42 MB (0.3% of available memory)\n","\u001b[36m(_dystack pid=189)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tStage 1 Generators:\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tStage 2 Generators:\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n","\u001b[36m(_dystack pid=189)\u001b[0m \tStage 3 Generators:\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting TextSpecialFeatureGenerator...\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t\tFitting BinnedFeatureGenerator...\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t\tFitting DropDuplicatesFeatureGenerator...\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting TextNgramFeatureGenerator...\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t\tFitting CountVectorizer for text features: ['engine']\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t\tCountVectorizer fit with vocabulary size = 116\n","\u001b[36m(_dystack pid=189)\u001b[0m \tStage 4 Generators:\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n","\u001b[36m(_dystack pid=189)\u001b[0m \tStage 5 Generators:\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n","\u001b[36m(_dystack pid=189)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', [])          : 2 | ['model_year', 'milage']\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t('object', [])       : 8 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t('object', ['text']) : 1 | ['engine']\n","\u001b[36m(_dystack pid=189)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t('category', [])                    :  7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t('category', ['text_as_category'])  :  1 | ['engine']\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', [])                         :  2 | ['model_year', 'milage']\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', ['bool'])                   :  1 | ['clean_title']\n","\u001b[36m(_dystack pid=189)\u001b[0m \t\t('int', ['text_ngram'])             : 63 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n","\u001b[36m(_dystack pid=189)\u001b[0m \t17.3s = Fit runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t11 features in original data used to generate 85 features in processed data.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tTrain Data (Processed) Memory Usage: 26.53 MB (0.1% of available memory)\n","\u001b[36m(_dystack pid=189)\u001b[0m Data preprocessing and feature engineering runtime = 17.37s ...\n","\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n","\u001b[36m(_dystack pid=189)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n","\u001b[36m(_dystack pid=189)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n","\u001b[36m(_dystack pid=189)\u001b[0m User-specified model hyperparameters to be fit:\n","\u001b[36m(_dystack pid=189)\u001b[0m {\n","\u001b[36m(_dystack pid=189)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n","\u001b[36m(_dystack pid=189)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\u001b[36m(_dystack pid=189)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n","\u001b[36m(_dystack pid=189)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n","\u001b[36m(_dystack pid=189)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n","\u001b[36m(_dystack pid=189)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\u001b[36m(_dystack pid=189)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\u001b[36m(_dystack pid=189)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","\u001b[36m(_dystack pid=189)\u001b[0m }\n","\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n","\u001b[36m(_dystack pid=189)\u001b[0m Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting 106 L1 models ...\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1784.81s of the 2677.87s of remaining time.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.63%)\n","\u001b[36m(_ray_fit pid=454)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=454)\u001b[0m [LightGBM] [Fatal] bin size 1669 cannot run on GPU\n","\u001b[36m(_ray_fit pid=454)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=496)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=496)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n","\u001b[36m(_ray_fit pid=496)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=538)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=538)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n","\u001b[36m(_ray_fit pid=538)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=580)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=580)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n","\u001b[36m(_ray_fit pid=580)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=622)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=622)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n","\u001b[36m(_ray_fit pid=622)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=664)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=664)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n","\u001b[36m(_ray_fit pid=664)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=706)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=706)\u001b[0m [LightGBM] [Fatal] bin size 1664 cannot run on GPU\n","\u001b[36m(_ray_fit pid=706)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=748)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=748)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n","\u001b[36m(_ray_fit pid=748)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-73344.2279\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t49.39s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t0.47s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 1730.07s of the 2623.13s of remaining time.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.64%)\n","\u001b[36m(_ray_fit pid=880)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=880)\u001b[0m [LightGBM] [Fatal] bin size 1669 cannot run on GPU\n","\u001b[36m(_ray_fit pid=880)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=922)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=922)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n","\u001b[36m(_ray_fit pid=922)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=964)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=964)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n","\u001b[36m(_ray_fit pid=964)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=1006)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=1006)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n","\u001b[36m(_ray_fit pid=1006)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=1048)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=1048)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n","\u001b[36m(_ray_fit pid=1048)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=1090)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=1090)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n","\u001b[36m(_ray_fit pid=1090)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=1132)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=1132)\u001b[0m [LightGBM] [Fatal] bin size 1664 cannot run on GPU\n","\u001b[36m(_ray_fit pid=1132)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=1174)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=1174)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n","\u001b[36m(_ray_fit pid=1174)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-73698.666\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t47.7s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t0.31s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 1679.86s of the 2572.92s of remaining time.\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-79007.3345\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t256.84s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t10.78s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 1411.03s of the 2304.08s of remaining time.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.69%)\n","\u001b[36m(_ray_fit pid=1320)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=1320)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n","\u001b[36m(_ray_fit pid=1396)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=1396)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n","\u001b[36m(_ray_fit pid=1472)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=1472)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n","\u001b[36m(_ray_fit pid=1548)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=1548)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n","\u001b[36m(_ray_fit pid=1625)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=1625)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n","\u001b[36m(_ray_fit pid=1701)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=1701)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n","\u001b[36m(_ray_fit pid=1777)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=1777)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n","\u001b[36m(_ray_fit pid=1853)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=1853)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-73231.253\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t95.67s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t1.01s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 1312.92s of the 2205.98s of remaining time.\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-77866.2078\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t184.42s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t10.43s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1116.8s of the 2009.86s of remaining time.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=1.03%)\n","\u001b[36m(_ray_fit pid=2033)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=2033)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n","\u001b[36m(_ray_fit pid=2086)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=2086)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n","\u001b[36m(_ray_fit pid=2139)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 14)\n","\u001b[36m(_ray_fit pid=2139)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=2139)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n","\u001b[36m(_ray_fit pid=2192)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=2192)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n","\u001b[36m(_ray_fit pid=2245)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=2245)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n","\u001b[36m(_ray_fit pid=2298)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=2298)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n","\u001b[36m(_ray_fit pid=2351)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=2351)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n","\u001b[36m(_ray_fit pid=2404)\u001b[0m /opt/conda/lib/python3.10/site-packages/fastai/learner.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=2404)\u001b[0m   state = torch.load(file, map_location=device, **torch_load_kwargs)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-73663.6119\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t818.19s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t3.11s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 295.89s of the 1188.95s of remaining time.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.91%)\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:21:53] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m   warnings.warn(smsg, UserWarning)\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:21:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m \n","\u001b[36m(_ray_fit pid=2547)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m \n","\u001b[36m(_ray_fit pid=2547)\u001b[0m   warnings.warn(smsg, UserWarning)\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:21:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m \n","\u001b[36m(_ray_fit pid=2547)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m \n","\u001b[36m(_ray_fit pid=2547)\u001b[0m   warnings.warn(smsg, UserWarning)\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:21:54] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m Potential solutions:\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m - Set the device for booster before call to inplace_predict.\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m \n","\u001b[36m(_ray_fit pid=2547)\u001b[0m This warning will only be shown once.\n","\u001b[36m(_ray_fit pid=2547)\u001b[0m \n","\u001b[36m(_ray_fit pid=2547)\u001b[0m   warnings.warn(smsg, UserWarning)\n","\u001b[36m(_ray_fit pid=2590)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:21:58] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n","\u001b[36m(_ray_fit pid=2590)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n","\u001b[36m(_ray_fit pid=2590)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:21:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n","\u001b[36m(_ray_fit pid=2590)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2590)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n","\u001b[36m(_ray_fit pid=2590)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:21:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n","\u001b[36m(_ray_fit pid=2590)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n","\u001b[36m(_ray_fit pid=2590)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:21:59] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n","\u001b[36m(_ray_fit pid=2590)\u001b[0m Potential solutions:\n","\u001b[36m(_ray_fit pid=2590)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n","\u001b[36m(_ray_fit pid=2590)\u001b[0m - Set the device for booster before call to inplace_predict.\n","\u001b[36m(_ray_fit pid=2590)\u001b[0m This warning will only be shown once.\n","\u001b[36m(_ray_fit pid=2634)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:03] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n","\u001b[36m(_ray_fit pid=2634)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2634)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2634)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:04] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n","\u001b[36m(_ray_fit pid=2634)\u001b[0m Potential solutions:\n","\u001b[36m(_ray_fit pid=2634)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n","\u001b[36m(_ray_fit pid=2634)\u001b[0m - Set the device for booster before call to inplace_predict.\n","\u001b[36m(_ray_fit pid=2634)\u001b[0m This warning will only be shown once.\n","\u001b[36m(_ray_fit pid=2678)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 3x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2678)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 3x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2678)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:08] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n","\u001b[36m(_ray_fit pid=2678)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 6x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2678)\u001b[0m \u001b[32m [repeated 10x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2721)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:13] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n","\u001b[36m(_ray_fit pid=2678)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n","\u001b[36m(_ray_fit pid=2678)\u001b[0m Potential solutions:\n","\u001b[36m(_ray_fit pid=2678)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n","\u001b[36m(_ray_fit pid=2678)\u001b[0m - Set the device for booster before call to inplace_predict.\n","\u001b[36m(_ray_fit pid=2678)\u001b[0m This warning will only be shown once.\n","\u001b[36m(_ray_fit pid=2721)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2721)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2721)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:14] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n","\u001b[36m(_ray_fit pid=2721)\u001b[0m Potential solutions:\n","\u001b[36m(_ray_fit pid=2721)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n","\u001b[36m(_ray_fit pid=2721)\u001b[0m - Set the device for booster before call to inplace_predict.\n","\u001b[36m(_ray_fit pid=2721)\u001b[0m This warning will only be shown once.\n","\u001b[36m(_ray_fit pid=2721)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2721)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2764)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:18] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n","\u001b[36m(_ray_fit pid=2764)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2764)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2764)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:19] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n","\u001b[36m(_ray_fit pid=2764)\u001b[0m Potential solutions:\n","\u001b[36m(_ray_fit pid=2764)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n","\u001b[36m(_ray_fit pid=2764)\u001b[0m - Set the device for booster before call to inplace_predict.\n","\u001b[36m(_ray_fit pid=2764)\u001b[0m This warning will only be shown once.\n","\u001b[36m(_ray_fit pid=2764)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2764)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2808)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:24] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n","\u001b[36m(_ray_fit pid=2808)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2808)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2808)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:24] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n","\u001b[36m(_ray_fit pid=2808)\u001b[0m Potential solutions:\n","\u001b[36m(_ray_fit pid=2808)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n","\u001b[36m(_ray_fit pid=2808)\u001b[0m - Set the device for booster before call to inplace_predict.\n","\u001b[36m(_ray_fit pid=2808)\u001b[0m This warning will only be shown once.\n","\u001b[36m(_ray_fit pid=2808)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2808)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-74112.2744\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t38.51s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t0.85s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 254.68s of the 1147.74s of remaining time.\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:28] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:29] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m Potential solutions:\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m - Set the device for booster before call to inplace_predict.\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m This warning will only be shown once.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.57%)\n","\u001b[36m(_ray_fit pid=2985)\u001b[0m TabularNeuralNetTorchModel not yet able to use more than 1 GPU. 'num_gpus' is set to >1, but we will be using only 1 GPU.\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m /opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:22:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2852)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n","\u001b[36m(_ray_fit pid=2985)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 9)\n","\u001b[36m(_ray_fit pid=2985)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=2985)\u001b[0m   self.model = torch.load(net_filename)\n","\u001b[36m(_ray_fit pid=3032)\u001b[0m TabularNeuralNetTorchModel not yet able to use more than 1 GPU. 'num_gpus' is set to >1, but we will be using only 1 GPU.\n","\u001b[36m(_ray_fit pid=3032)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 9)\n","\u001b[36m(_ray_fit pid=3032)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=3032)\u001b[0m   self.model = torch.load(net_filename)\n","\u001b[36m(_ray_fit pid=3079)\u001b[0m TabularNeuralNetTorchModel not yet able to use more than 1 GPU. 'num_gpus' is set to >1, but we will be using only 1 GPU.\n","\u001b[36m(_ray_fit pid=3079)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 9)\n","\u001b[36m(_ray_fit pid=3079)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=3079)\u001b[0m   self.model = torch.load(net_filename)\n","\u001b[36m(_ray_fit pid=3126)\u001b[0m TabularNeuralNetTorchModel not yet able to use more than 1 GPU. 'num_gpus' is set to >1, but we will be using only 1 GPU.\n","\u001b[36m(_ray_fit pid=3126)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 9)\n","\u001b[36m(_ray_fit pid=3126)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=3126)\u001b[0m   self.model = torch.load(net_filename)\n","\u001b[36m(_ray_fit pid=3173)\u001b[0m TabularNeuralNetTorchModel not yet able to use more than 1 GPU. 'num_gpus' is set to >1, but we will be using only 1 GPU.\n","\u001b[36m(_ray_fit pid=3173)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 9)\n","\u001b[36m(_ray_fit pid=3173)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=3173)\u001b[0m   self.model = torch.load(net_filename)\n","\u001b[36m(_ray_fit pid=3220)\u001b[0m TabularNeuralNetTorchModel not yet able to use more than 1 GPU. 'num_gpus' is set to >1, but we will be using only 1 GPU.\n","\u001b[36m(_ray_fit pid=3220)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 9)\n","\u001b[36m(_ray_fit pid=3220)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=3220)\u001b[0m   self.model = torch.load(net_filename)\n","\u001b[36m(_ray_fit pid=3267)\u001b[0m TabularNeuralNetTorchModel not yet able to use more than 1 GPU. 'num_gpus' is set to >1, but we will be using only 1 GPU.\n","\u001b[36m(_ray_fit pid=3267)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 9)\n","\u001b[36m(_ray_fit pid=3267)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=3267)\u001b[0m   self.model = torch.load(net_filename)\n","\u001b[36m(_ray_fit pid=3314)\u001b[0m TabularNeuralNetTorchModel not yet able to use more than 1 GPU. 'num_gpus' is set to >1, but we will be using only 1 GPU.\n","\u001b[36m(_ray_fit pid=3314)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 9)\n","\u001b[36m(_ray_fit pid=3314)\u001b[0m /opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py:411: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_ray_fit pid=3314)\u001b[0m   self.model = torch.load(net_filename)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-74207.4918\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t225.95s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t2.14s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 26.26s of the 919.32s of remaining time.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.82%)\n","\u001b[36m(_ray_fit pid=3451)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=3451)\u001b[0m [LightGBM] [Fatal] bin size 1669 cannot run on GPU\n","\u001b[36m(_ray_fit pid=3451)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=3451)\u001b[0m \tRan out of time, early stopping on iteration 70. Best iteration is:\n","\u001b[36m(_ray_fit pid=3451)\u001b[0m \t[61]\tvalid_set's rmse: 82621.6\n","\u001b[36m(_ray_fit pid=3493)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=3493)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n","\u001b[36m(_ray_fit pid=3493)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=3493)\u001b[0m \tRan out of time, early stopping on iteration 75. Best iteration is:\n","\u001b[36m(_ray_fit pid=3493)\u001b[0m \t[60]\tvalid_set's rmse: 84131.4\n","\u001b[36m(_ray_fit pid=3535)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=3535)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n","\u001b[36m(_ray_fit pid=3535)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=3577)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=3577)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n","\u001b[36m(_ray_fit pid=3577)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=3577)\u001b[0m \tRan out of time, early stopping on iteration 73. Best iteration is:\n","\u001b[36m(_ray_fit pid=3577)\u001b[0m \t[52]\tvalid_set's rmse: 79682.1\n","\u001b[36m(_ray_fit pid=3619)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=3619)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n","\u001b[36m(_ray_fit pid=3619)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=3619)\u001b[0m \tRan out of time, early stopping on iteration 75. Best iteration is:\n","\u001b[36m(_ray_fit pid=3619)\u001b[0m \t[59]\tvalid_set's rmse: 68476.5\n","\u001b[36m(_ray_fit pid=3661)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=3661)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n","\u001b[36m(_ray_fit pid=3661)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=3661)\u001b[0m \tRan out of time, early stopping on iteration 76. Best iteration is:\n","\u001b[36m(_ray_fit pid=3661)\u001b[0m \t[72]\tvalid_set's rmse: 70483.5\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m [LightGBM] [Fatal] bin size 1664 cannot run on GPU\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m \tRan out of time, early stopping on iteration 49. Best iteration is:\n","\u001b[36m(_ray_fit pid=3703)\u001b[0m \t[49]\tvalid_set's rmse: 62449.1\n","\u001b[36m(_ray_fit pid=3745)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=3745)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n","\u001b[36m(_ray_fit pid=3745)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=3745)\u001b[0m \tRan out of time, early stopping on iteration 69. Best iteration is:\n","\u001b[36m(_ray_fit pid=3745)\u001b[0m \t[60]\tvalid_set's rmse: 71449.8\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-74114.7128\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t53.6s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t0.49s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 863.22s of remaining time.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L1': 0.5, 'NeuralNetFastAI_BAG_L1': 0.208, 'LightGBMXT_BAG_L1': 0.167, 'LightGBM_BAG_L1': 0.042, 'RandomForestMSE_BAG_L1': 0.042, 'ExtraTreesMSE_BAG_L1': 0.042}\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-73074.434\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t0.26s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t0.0s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting 106 L2 models ...\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 862.93s of the 862.86s of remaining time.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.77%)\n","\u001b[36m(_ray_fit pid=3883)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=3883)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n","\u001b[36m(_ray_fit pid=3883)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=3925)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=3925)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n","\u001b[36m(_ray_fit pid=3925)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=3967)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=3967)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n","\u001b[36m(_ray_fit pid=3967)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=4009)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=4009)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n","\u001b[36m(_ray_fit pid=4009)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=4051)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=4051)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n","\u001b[36m(_ray_fit pid=4051)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=4093)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=4093)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n","\u001b[36m(_ray_fit pid=4093)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=4135)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=4135)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n","\u001b[36m(_ray_fit pid=4135)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=4177)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=4177)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n","\u001b[36m(_ray_fit pid=4177)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-73098.6517\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t54.63s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t0.4s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 805.83s of the 805.75s of remaining time.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.77%)\n","\u001b[36m(_ray_fit pid=4315)\u001b[0m \tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=4315)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n","\u001b[36m(_ray_fit pid=4315)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=4357)\u001b[0m \tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=4357)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n","\u001b[36m(_ray_fit pid=4357)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=4399)\u001b[0m \tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=4399)\u001b[0m [LightGBM] [Fatal] bin size 1668 cannot run on GPU\n","\u001b[36m(_ray_fit pid=4399)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=4441)\u001b[0m \tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=4441)\u001b[0m [LightGBM] [Fatal] bin size 1666 cannot run on GPU\n","\u001b[36m(_ray_fit pid=4441)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=4483)\u001b[0m \tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=4483)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n","\u001b[36m(_ray_fit pid=4483)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=4525)\u001b[0m \tTraining S1F6 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=4525)\u001b[0m [LightGBM] [Fatal] bin size 1670 cannot run on GPU\n","\u001b[36m(_ray_fit pid=4525)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=4567)\u001b[0m \tTraining S1F7 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=4567)\u001b[0m [LightGBM] [Fatal] bin size 1665 cannot run on GPU\n","\u001b[36m(_ray_fit pid=4567)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_ray_fit pid=4609)\u001b[0m \tTraining S1F8 with GPU, note that this may negatively impact model quality compared to CPU training.\n","\u001b[36m(_ray_fit pid=4609)\u001b[0m [LightGBM] [Fatal] bin size 1667 cannot run on GPU\n","\u001b[36m(_ray_fit pid=4609)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-73477.4844\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t54.67s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t0.36s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 748.59s of the 748.51s of remaining time.\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-74862.8544\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t732.96s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t12.38s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 1.16s of remaining time.\n","\u001b[36m(_dystack pid=189)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.4, 'CatBoost_BAG_L1': 0.32, 'NeuralNetFastAI_BAG_L1': 0.12, 'RandomForestMSE_BAG_L2': 0.08, 'ExtraTreesMSE_BAG_L1': 0.04, 'LightGBM_BAG_L2': 0.04}\n","\u001b[36m(_dystack pid=189)\u001b[0m \t-72993.4848\t = Validation score   (-root_mean_squared_error)\n","\u001b[36m(_dystack pid=189)\u001b[0m \t0.31s\t = Training   runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m \t0.0s\t = Validation runtime\n","\u001b[36m(_dystack pid=189)\u001b[0m AutoGluon training complete, total runtime = 2694.48s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1570.1 rows/s (20948 batch size)\n","\u001b[36m(_dystack pid=189)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240910_145635/ds_sub_fit/sub_fit_ho\")\n","\u001b[36m(_dystack pid=189)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_dystack pid=189)\u001b[0m   return torch.load(io.BytesIO(b))\n","\u001b[36m(_dystack pid=189)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_dystack pid=189)\u001b[0m   return torch.load(io.BytesIO(b))\n","\u001b[36m(_dystack pid=189)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_dystack pid=189)\u001b[0m   return torch.load(io.BytesIO(b))\n","\u001b[36m(_dystack pid=189)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_dystack pid=189)\u001b[0m   return torch.load(io.BytesIO(b))\n","\u001b[36m(_dystack pid=189)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_dystack pid=189)\u001b[0m   return torch.load(io.BytesIO(b))\n","\u001b[36m(_dystack pid=189)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_dystack pid=189)\u001b[0m   return torch.load(io.BytesIO(b))\n","\u001b[36m(_dystack pid=189)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_dystack pid=189)\u001b[0m   return torch.load(io.BytesIO(b))\n","\u001b[36m(_dystack pid=189)\u001b[0m /opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","\u001b[36m(_dystack pid=189)\u001b[0m   return torch.load(io.BytesIO(b))\n","\u001b[36m(_dystack pid=189)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n","Leaderboard on holdout data (DyStack):\n","                     model  score_holdout     score_val              eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n","0      WeightedEnsemble_L3  -69641.028314 -72993.484790  root_mean_squared_error       17.613206      42.744801  2612.843060                 0.004883                0.002423           0.314665            3       True         14\n","1        LightGBMXT_BAG_L2  -69670.584465 -73098.651740  root_mean_squared_error       15.548771      30.000560  1824.892636                 0.522939                0.399479          54.631836            2       True         11\n","2      WeightedEnsemble_L2  -69725.735650 -73074.433990  root_mean_squared_error       11.695720      26.119827  1452.464957                 0.004659                0.002834           0.258574            2       True         10\n","3   NeuralNetFastAI_BAG_L1  -69862.218069 -73663.611863  root_mean_squared_error        6.388598       3.107702   818.190462                 6.388598                3.107702         818.190462            1       True          6\n","4          CatBoost_BAG_L1  -69876.460704 -73231.253036  root_mean_squared_error        0.929191       1.011724    95.666765                 0.929191                1.011724          95.666765            1       True          4\n","5        LightGBMXT_BAG_L1  -69893.921998 -73344.227854  root_mean_squared_error        1.011068       0.467942    49.392227                 1.011068                0.467942          49.392227            1       True          1\n","6          LightGBM_BAG_L2  -69901.776459 -73477.484440  root_mean_squared_error       15.433496      29.958751  1824.932798                 0.407664                0.357671          54.671999            2       True         12\n","7          LightGBM_BAG_L1  -70168.614143 -73698.665969  root_mean_squared_error        0.398901       0.312648    47.701389                 0.398901                0.312648          47.701389            1       True          2\n","8     LightGBMLarge_BAG_L1  -70645.477596 -74114.712777  root_mean_squared_error        0.705891       0.488533    53.596657                 0.705891                0.488533          53.596657            1       True          9\n","9           XGBoost_BAG_L1  -70739.212438 -74112.274361  root_mean_squared_error        1.190575       0.853719    38.511826                 1.190575                0.853719          38.511826            1       True          7\n","10   NeuralNetTorch_BAG_L1  -70847.059228 -74207.491773  root_mean_squared_error        1.438305       2.141837   225.945933                 1.438305                2.141837         225.945933            1       True          8\n","11  RandomForestMSE_BAG_L2  -71000.999331 -74862.854409  root_mean_squared_error       16.677720      41.985228  2503.224560                 1.651888               12.384147         732.963761            2       True         13\n","12    ExtraTreesMSE_BAG_L1  -74374.745208 -77866.207816  root_mean_squared_error        1.451649      10.434166   184.418579                 1.451649               10.434166         184.418579            1       True          5\n","13  RandomForestMSE_BAG_L1  -74756.750552 -79007.334504  root_mean_squared_error        1.511654      10.782811   256.836962                 1.511654               10.782811         256.836962            1       True          3\n","\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n","\t2721s\t = DyStack   runtime |\t8079s\t = Remaining runtime\n","Starting main fit with num_stack_levels=1.\n","\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n","Beginning AutoGluon training ... Time limit = 8079s\n","AutoGluon will save models to \"AutogluonModels/ag-20240910_145635\"\n","Train Data Rows:    188533\n","Train Data Columns: 11\n","Label Column:       price\n","Problem Type:       regression\n","Preprocessing data ...\n","Using Feature Generators to preprocess the data ...\n","Fitting AutoMLPipelineFeatureGenerator...\n","\tAvailable Memory:                    29783.13 MB\n","\tTrain Data (Original)  Memory Usage: 115.28 MB (0.4% of available memory)\n","\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n","\tStage 1 Generators:\n","\t\tFitting AsTypeFeatureGenerator...\n","\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n","\tStage 2 Generators:\n","\t\tFitting FillNaFeatureGenerator...\n","\tStage 3 Generators:\n","\t\tFitting IdentityFeatureGenerator...\n","\t\tFitting CategoryFeatureGenerator...\n","\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n","\t\tFitting TextSpecialFeatureGenerator...\n","\t\t\tFitting BinnedFeatureGenerator...\n","\t\t\tFitting DropDuplicatesFeatureGenerator...\n","\t\tFitting TextNgramFeatureGenerator...\n","\t\t\tFitting CountVectorizer for text features: ['engine']\n","\t\t\tCountVectorizer fit with vocabulary size = 116\n","\tStage 4 Generators:\n","\t\tFitting DropUniqueFeatureGenerator...\n","\tStage 5 Generators:\n","\t\tFitting DropDuplicatesFeatureGenerator...\n","\tTypes of features in original data (raw dtype, special dtypes):\n","\t\t('int', [])          : 2 | ['model_year', 'milage']\n","\t\t('object', [])       : 8 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n","\t\t('object', ['text']) : 1 | ['engine']\n","\tTypes of features in processed data (raw dtype, special dtypes):\n","\t\t('category', [])                    :  7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n","\t\t('category', ['text_as_category'])  :  1 | ['engine']\n","\t\t('int', [])                         :  2 | ['model_year', 'milage']\n","\t\t('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n","\t\t('int', ['bool'])                   :  1 | ['clean_title']\n","\t\t('int', ['text_ngram'])             : 65 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n","\t18.6s = Fit runtime\n","\t11 features in original data used to generate 87 features in processed data.\n","\tTrain Data (Processed) Memory Usage: 30.57 MB (0.1% of available memory)\n","Data preprocessing and feature engineering runtime = 18.73s ...\n","AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n","\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n","\tTo change this, specify the eval_metric parameter of Predictor()\n","Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n","User-specified model hyperparameters to be fit:\n","{\n","\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n","\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n","\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n","\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n","\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n","\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n","\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n","}\n","AutoGluon will fit 2 stack levels (L1 to L2) ...\n","Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n","Fitting 106 L1 models ...\n","Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5372.22s of the 8060.33s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.69%)\n","\t-72917.4814\t = Validation score   (-root_mean_squared_error)\n","\t56.37s\t = Training   runtime\n","\t0.63s\t = Validation runtime\n","Fitting model: LightGBM_BAG_L1 ... Training model for up to 5311.5s of the 7999.6s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.70%)\n","\t-73211.0061\t = Validation score   (-root_mean_squared_error)\n","\t52.22s\t = Training   runtime\n","\t0.42s\t = Validation runtime\n","Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 5256.68s of the 7944.79s of remaining time.\n","\t-78153.0079\t = Validation score   (-root_mean_squared_error)\n","\t311.9s\t = Training   runtime\n","\t12.18s\t = Validation runtime\n","Fitting model: CatBoost_BAG_L1 ... Training model for up to 4931.29s of the 7619.39s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.75%)\n","\t-72812.7961\t = Validation score   (-root_mean_squared_error)\n","\t120.78s\t = Training   runtime\n","\t1.24s\t = Validation runtime\n","Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 4807.86s of the 7495.97s of remaining time.\n","\t-77344.0729\t = Validation score   (-root_mean_squared_error)\n","\t234.75s\t = Training   runtime\n","\t11.66s\t = Validation runtime\n","Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 4560.04s of the 7248.15s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=1.12%)\n","\t-73204.8227\t = Validation score   (-root_mean_squared_error)\n","\t1500.94s\t = Training   runtime\n","\t3.48s\t = Validation runtime\n","Fitting model: XGBoost_BAG_L1 ... Training model for up to 3056.29s of the 5744.39s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.98%)\n","\t-73649.3785\t = Validation score   (-root_mean_squared_error)\n","\t41.48s\t = Training   runtime\n","\t1.04s\t = Validation runtime\n","Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 3012.04s of the 5700.15s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.62%)\n","\t-73419.6797\t = Validation score   (-root_mean_squared_error)\n","\t1266.87s\t = Training   runtime\n","\t1.21s\t = Validation runtime\n","Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1742.84s of the 4430.94s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.88%)\n","\t-73620.199\t = Validation score   (-root_mean_squared_error)\n","\t61.91s\t = Training   runtime\n","\t0.58s\t = Validation runtime\n","Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1678.39s of the 4366.5s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.76%)\n","\t-72844.3161\t = Validation score   (-root_mean_squared_error)\n","\t89.38s\t = Training   runtime\n","\t0.98s\t = Validation runtime\n","Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 1586.31s of the 4274.42s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.63%)\n","\t-73457.8949\t = Validation score   (-root_mean_squared_error)\n","\t921.75s\t = Training   runtime\n","\t1.36s\t = Validation runtime\n","Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 661.96s of the 3350.07s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.75%)\n","\t-73119.0106\t = Validation score   (-root_mean_squared_error)\n","\t87.94s\t = Training   runtime\n","\t1.36s\t = Validation runtime\n","Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 571.21s of the 3259.32s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=1.14%)\n","\t-73257.5465\t = Validation score   (-root_mean_squared_error)\n","\t499.0s\t = Training   runtime\n","\t3.56s\t = Validation runtime\n","Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 69.2s of the 2757.3s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=1.11%)\n","\t-75739.0011\t = Validation score   (-root_mean_squared_error)\n","\t43.73s\t = Training   runtime\n","\t0.47s\t = Validation runtime\n","Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 22.88s of the 2710.98s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.68%)\n","\t-74312.9392\t = Validation score   (-root_mean_squared_error)\n","\t51.48s\t = Training   runtime\n","\t0.68s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L2 ... Training model for up to 537.22s of the 2656.55s of remaining time.\n","\tEnsemble Weights: {'CatBoost_BAG_L1': 0.375, 'NeuralNetFastAI_BAG_L1': 0.167, 'NeuralNetFastAI_r191_BAG_L1': 0.125, 'LightGBMXT_BAG_L1': 0.083, 'NeuralNetTorch_r79_BAG_L1': 0.083, 'LightGBM_BAG_L1': 0.042, 'RandomForestMSE_BAG_L1': 0.042, 'ExtraTreesMSE_BAG_L1': 0.042, 'LightGBM_r131_BAG_L1': 0.042}\n","\t-72600.3396\t = Validation score   (-root_mean_squared_error)\n","\t0.47s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n","Fitting 106 L2 models ...\n","Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2656.02s of the 2655.89s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.93%)\n","\t-72603.1656\t = Validation score   (-root_mean_squared_error)\n","\t90.14s\t = Training   runtime\n","\t1.68s\t = Validation runtime\n","Fitting model: LightGBM_BAG_L2 ... Training model for up to 2563.16s of the 2563.03s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.94%)\n","\t-73042.741\t = Validation score   (-root_mean_squared_error)\n","\t60.67s\t = Training   runtime\n","\t0.39s\t = Validation runtime\n","Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 2499.66s of the 2499.53s of remaining time.\n","\t-74471.5472\t = Validation score   (-root_mean_squared_error)\n","\t1179.53s\t = Training   runtime\n","\t15.61s\t = Validation runtime\n","Fitting model: CatBoost_BAG_L2 ... Training model for up to 1303.12s of the 1303.0s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.99%)\n","\t-72542.7972\t = Validation score   (-root_mean_squared_error)\n","\t77.33s\t = Training   runtime\n","\t0.65s\t = Validation runtime\n","Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 1223.02s of the 1222.89s of remaining time.\n","\t-74194.4831\t = Validation score   (-root_mean_squared_error)\n","\t326.96s\t = Training   runtime\n","\t14.12s\t = Validation runtime\n","Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 880.56s of the 880.43s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=1.52%)\n","\t-72879.9229\t = Validation score   (-root_mean_squared_error)\n","\t657.71s\t = Training   runtime\n","\t3.5s\t = Validation runtime\n","Fitting model: XGBoost_BAG_L2 ... Training model for up to 219.85s of the 219.72s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=1.31%)\n","\t-72995.0758\t = Validation score   (-root_mean_squared_error)\n","\t47.66s\t = Training   runtime\n","\t1.28s\t = Validation runtime\n","Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 169.34s of the 169.21s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=0.85%)\n","\t-73042.7848\t = Validation score   (-root_mean_squared_error)\n","\t156.33s\t = Training   runtime\n","\t1.95s\t = Validation runtime\n","Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 10.18s of the 10.05s of remaining time.\n","\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=4, gpus=2, memory=1.16%)\n","\t-78476.8789\t = Validation score   (-root_mean_squared_error)\n","\t43.7s\t = Training   runtime\n","\t0.23s\t = Validation runtime\n","Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -37.44s of remaining time.\n","\tEnsemble Weights: {'LightGBMXT_BAG_L2': 0.4, 'CatBoost_BAG_L2': 0.4, 'NeuralNetFastAI_BAG_L1': 0.04, 'NeuralNetTorch_r79_BAG_L1': 0.04, 'NeuralNetFastAI_r191_BAG_L1': 0.04, 'RandomForestMSE_BAG_L2': 0.04, 'ExtraTreesMSE_BAG_L2': 0.04}\n","\t-72449.2538\t = Validation score   (-root_mean_squared_error)\n","\t0.56s\t = Training   runtime\n","\t0.0s\t = Validation runtime\n","AutoGluon training complete, total runtime = 8117.17s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 905.1 rows/s (23567 batch size)\n","TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240910_145635\")\n"]}],"source":["predictor = TabularPredictor(\n","    label='price',             # Target column that needs to be predicted (dependent variable)\n","    eval_metric='rmse',        # Evaluation metric (Root Mean Squared Error) used to judge the model’s performance\n","    problem_type='regression'  # Specifying this is a regression problem\n",").fit(\n","    train_data,                  # The training dataset containing features and the target (price)\n","    presets='best_quality',    # The preset configuration for optimal quality (though it may take more time)\n","    time_limit=3600*3,      # Time limit for training (3 hours = 3600 seconds/hour * 3 hours)\n","    verbosity=2,               # Level of logging information (2 is medium verbosity)\n","    excluded_model_types=['KNN'], # Exclude K-Nearest Neighbors models from training\n","    ag_args_fit={\n","        'num_gpus': 2,          # Use 2 GPUs if available for model training\n","        'num_cpus': 4           # Use 4 CPUs for model training\n","    }\n",")"]},{"cell_type":"markdown","id":"d6354791","metadata":{"papermill":{"duration":0.042298,"end_time":"2024-09-10T17:57:13.680586","exception":false,"start_time":"2024-09-10T17:57:13.638288","status":"completed"},"tags":[]},"source":["* **label='price':** The column name 'price' is the target (dependent variable) to be predicted.\n","* **eval_metric='rmse':** The Root Mean Squared Error (RMSE) is chosen as the evaluation metric, which is common for regression tasks.\n","* **problem_type='regression':** Specifies that the task is a regression task (i.e., predicting continuous values).\n","* **train_data:** This is the DataFrame containing the training data with both features and the target (price).\n","* **presets='best_quality':** This preset prioritizes accuracy over training speed. It will try many models and techniques to ensure the highest possible quality.\n","* **time_limit=3600*10:** Limits the model training process to a maximum of 10 hours.\n","* **verbosity=2:** Specifies the verbosity level for logging. Higher values will show more details about the training process.\n","* **excluded_model_types=['KNN']:** K-Nearest Neighbors (KNN) models are excluded from being considered during training.\n","* **ag_args_fit:** This argument allows you to pass configuration options to the fitting process:\n","* **num_gpus=2:** The model will utilize 2 GPUs for training if available, speeding up the process for certain algorithms.\n","* **num_cpus=4:** The model will use 4 CPU cores during training."]},{"cell_type":"code","execution_count":7,"id":"0de6f116","metadata":{"execution":{"iopub.execute_input":"2024-09-10T17:57:13.767447Z","iopub.status.busy":"2024-09-10T17:57:13.765978Z","iopub.status.idle":"2024-09-10T17:57:14.533102Z","shell.execute_reply":"2024-09-10T17:57:14.532155Z"},"papermill":{"duration":0.812513,"end_time":"2024-09-10T17:57:14.535453","exception":false,"start_time":"2024-09-10T17:57:13.72294","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["*** Summary of fit() ***\n","Estimated performance of each model:\n","                          model     score_val              eval_metric  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n","0           WeightedEnsemble_L3 -72449.253754  root_mean_squared_error      72.923016  7015.015082                0.002833           0.555356            3       True         26\n","1               CatBoost_BAG_L2 -72542.797184  root_mean_squared_error      41.507642  5417.826240                0.653220          77.326773            2       True         20\n","2           WeightedEnsemble_L2 -72600.339591  root_mean_squared_error      35.898734  3786.125246                0.003237           0.470863            2       True         16\n","3             LightGBMXT_BAG_L2 -72603.165649  root_mean_squared_error      42.533028  5430.640810                1.678606          90.141343            2       True         17\n","4               CatBoost_BAG_L1 -72812.796073  root_mean_squared_error       1.244417   120.778952                1.244417         120.778952            1       True          4\n","5          CatBoost_r177_BAG_L1 -72844.316077  root_mean_squared_error       0.979802    89.376491                0.979802          89.376491            1       True         10\n","6        NeuralNetFastAI_BAG_L2 -72879.922868  root_mean_squared_error      44.351679  5998.206054                3.497256         657.706587            2       True         22\n","7             LightGBMXT_BAG_L1 -72917.481425  root_mean_squared_error       0.631912    56.368863                0.631912          56.368863            1       True          1\n","8                XGBoost_BAG_L2 -72995.075779  root_mean_squared_error      42.137048  5388.159207                1.282625          47.659740            2       True         23\n","9               LightGBM_BAG_L2 -73042.741028  root_mean_squared_error      41.245911  5401.166497                0.391488          60.667030            2       True         18\n","10        NeuralNetTorch_BAG_L2 -73042.784785  root_mean_squared_error      42.807665  5496.833271                1.953242         156.333805            2       True         24\n","11         LightGBM_r131_BAG_L1 -73119.010616  root_mean_squared_error       1.360831    87.939971                1.360831          87.939971            1       True         12\n","12       NeuralNetFastAI_BAG_L1 -73204.822678  root_mean_squared_error       3.476512  1500.940777                3.476512        1500.940777            1       True          6\n","13              LightGBM_BAG_L1 -73211.006130  root_mean_squared_error       0.420324    52.221435                0.420324          52.221435            1       True          2\n","14  NeuralNetFastAI_r191_BAG_L1 -73257.546485  root_mean_squared_error       3.556240   499.003131                3.556240         499.003131            1       True         13\n","15        NeuralNetTorch_BAG_L1 -73419.679730  root_mean_squared_error       1.207690  1266.870634                1.207690        1266.870634            1       True          8\n","16    NeuralNetTorch_r79_BAG_L1 -73457.894931  root_mean_squared_error       1.359697   921.746454                1.359697         921.746454            1       True         11\n","17         LightGBMLarge_BAG_L1 -73620.198965  root_mean_squared_error       0.580270    61.912755                0.580270          61.912755            1       True          9\n","18               XGBoost_BAG_L1 -73649.378532  root_mean_squared_error       1.037074    41.478207                1.037074          41.478207            1       True          7\n","19         ExtraTreesMSE_BAG_L2 -74194.483143  root_mean_squared_error      54.974343  5667.460711               14.119920         326.961244            2       True         21\n","20          LightGBM_r96_BAG_L1 -74312.939239  root_mean_squared_error       0.680840    51.478891                0.680840          51.478891            1       True         15\n","21       RandomForestMSE_BAG_L2 -74471.547154  root_mean_squared_error      56.468437  6520.030366               15.614015        1179.530900            2       True         19\n","22           CatBoost_r9_BAG_L1 -75739.001120  root_mean_squared_error       0.473249    43.728106                0.473249          43.728106            1       True         14\n","23         ExtraTreesMSE_BAG_L1 -77344.072883  root_mean_squared_error      11.662408   234.754137               11.662408         234.754137            1       True          5\n","24       RandomForestMSE_BAG_L1 -78153.007850  root_mean_squared_error      12.183157   311.900662               12.183157         311.900662            1       True          3\n","25         LightGBMLarge_BAG_L2 -78476.878886  root_mean_squared_error      41.081681  5384.204360                0.227258          43.704893            2       True         25\n","Number of models trained: 26\n","Types of models trained:\n","{'StackerEnsembleModel_RF', 'StackerEnsembleModel_CatBoost', 'StackerEnsembleModel_TabularNeuralNetTorch', 'StackerEnsembleModel_NNFastAiTabular', 'WeightedEnsembleModel', 'StackerEnsembleModel_XGBoost', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_XT'}\n","Bagging used: True  (with 8 folds)\n","Multi-layer stack-ensembling used: True  (with 3 levels)\n","Feature Metadata (Processed):\n","(raw dtype, special dtypes):\n","('category', [])                    :  7 | ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', ...]\n","('category', ['text_as_category'])  :  1 | ['engine']\n","('int', [])                         :  2 | ['model_year', 'milage']\n","('int', ['binned', 'text_special']) : 11 | ['engine.char_count', 'engine.word_count', 'engine.capital_ratio', 'engine.lower_ratio', 'engine.digit_ratio', ...]\n","('int', ['bool'])                   :  1 | ['clean_title']\n","('int', ['text_ngram'])             : 65 | ['__nlp__.0hp', '__nlp__.0hp 0l', '__nlp__.0hp 0l cylinder', '__nlp__.0hp 0l straight', '__nlp__.0hp 0l v6', ...]\n","Plot summary of models saved to file: AutogluonModels/ag-20240910_145635SummaryOfModels.html\n","*** End of fit() summary ***\n"]}],"source":["results = predictor.fit_summary()"]},{"cell_type":"markdown","id":"70b0aa97","metadata":{"papermill":{"duration":0.0419,"end_time":"2024-09-10T17:57:14.620645","exception":false,"start_time":"2024-09-10T17:57:14.578745","status":"completed"},"tags":[]},"source":["* **fit_summary():** After training is complete, this method outputs a summary of the models trained, their performance, and additional statistics. The results object will contain information such as the leaderboard of model performance, training times, and which model was selected as the best for predictions."]},{"cell_type":"code","execution_count":8,"id":"906a4762","metadata":{"execution":{"iopub.execute_input":"2024-09-10T17:57:14.707483Z","iopub.status.busy":"2024-09-10T17:57:14.706708Z","iopub.status.idle":"2024-09-10T17:57:14.737048Z","shell.execute_reply":"2024-09-10T17:57:14.736061Z"},"papermill":{"duration":0.075853,"end_time":"2024-09-10T17:57:14.738945","exception":false,"start_time":"2024-09-10T17:57:14.663092","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>score_val</th>\n","      <th>eval_metric</th>\n","      <th>pred_time_val</th>\n","      <th>fit_time</th>\n","      <th>pred_time_val_marginal</th>\n","      <th>fit_time_marginal</th>\n","      <th>stack_level</th>\n","      <th>can_infer</th>\n","      <th>fit_order</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>WeightedEnsemble_L3</td>\n","      <td>-72449.253754</td>\n","      <td>root_mean_squared_error</td>\n","      <td>72.923016</td>\n","      <td>7015.015082</td>\n","      <td>0.002833</td>\n","      <td>0.555356</td>\n","      <td>3</td>\n","      <td>True</td>\n","      <td>26</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>CatBoost_BAG_L2</td>\n","      <td>-72542.797184</td>\n","      <td>root_mean_squared_error</td>\n","      <td>41.507642</td>\n","      <td>5417.826240</td>\n","      <td>0.653220</td>\n","      <td>77.326773</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>WeightedEnsemble_L2</td>\n","      <td>-72600.339591</td>\n","      <td>root_mean_squared_error</td>\n","      <td>35.898734</td>\n","      <td>3786.125246</td>\n","      <td>0.003237</td>\n","      <td>0.470863</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LightGBMXT_BAG_L2</td>\n","      <td>-72603.165649</td>\n","      <td>root_mean_squared_error</td>\n","      <td>42.533028</td>\n","      <td>5430.640810</td>\n","      <td>1.678606</td>\n","      <td>90.141343</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>CatBoost_BAG_L1</td>\n","      <td>-72812.796073</td>\n","      <td>root_mean_squared_error</td>\n","      <td>1.244417</td>\n","      <td>120.778952</td>\n","      <td>1.244417</td>\n","      <td>120.778952</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>CatBoost_r177_BAG_L1</td>\n","      <td>-72844.316077</td>\n","      <td>root_mean_squared_error</td>\n","      <td>0.979802</td>\n","      <td>89.376491</td>\n","      <td>0.979802</td>\n","      <td>89.376491</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>NeuralNetFastAI_BAG_L2</td>\n","      <td>-72879.922868</td>\n","      <td>root_mean_squared_error</td>\n","      <td>44.351679</td>\n","      <td>5998.206054</td>\n","      <td>3.497256</td>\n","      <td>657.706587</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>LightGBMXT_BAG_L1</td>\n","      <td>-72917.481425</td>\n","      <td>root_mean_squared_error</td>\n","      <td>0.631912</td>\n","      <td>56.368863</td>\n","      <td>0.631912</td>\n","      <td>56.368863</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>XGBoost_BAG_L2</td>\n","      <td>-72995.075779</td>\n","      <td>root_mean_squared_error</td>\n","      <td>42.137048</td>\n","      <td>5388.159207</td>\n","      <td>1.282625</td>\n","      <td>47.659740</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>LightGBM_BAG_L2</td>\n","      <td>-73042.741028</td>\n","      <td>root_mean_squared_error</td>\n","      <td>41.245911</td>\n","      <td>5401.166497</td>\n","      <td>0.391488</td>\n","      <td>60.667030</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>NeuralNetTorch_BAG_L2</td>\n","      <td>-73042.784785</td>\n","      <td>root_mean_squared_error</td>\n","      <td>42.807665</td>\n","      <td>5496.833271</td>\n","      <td>1.953242</td>\n","      <td>156.333805</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>24</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>LightGBM_r131_BAG_L1</td>\n","      <td>-73119.010616</td>\n","      <td>root_mean_squared_error</td>\n","      <td>1.360831</td>\n","      <td>87.939971</td>\n","      <td>1.360831</td>\n","      <td>87.939971</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>NeuralNetFastAI_BAG_L1</td>\n","      <td>-73204.822678</td>\n","      <td>root_mean_squared_error</td>\n","      <td>3.476512</td>\n","      <td>1500.940777</td>\n","      <td>3.476512</td>\n","      <td>1500.940777</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>LightGBM_BAG_L1</td>\n","      <td>-73211.006130</td>\n","      <td>root_mean_squared_error</td>\n","      <td>0.420324</td>\n","      <td>52.221435</td>\n","      <td>0.420324</td>\n","      <td>52.221435</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>NeuralNetFastAI_r191_BAG_L1</td>\n","      <td>-73257.546485</td>\n","      <td>root_mean_squared_error</td>\n","      <td>3.556240</td>\n","      <td>499.003131</td>\n","      <td>3.556240</td>\n","      <td>499.003131</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>NeuralNetTorch_BAG_L1</td>\n","      <td>-73419.679730</td>\n","      <td>root_mean_squared_error</td>\n","      <td>1.207690</td>\n","      <td>1266.870634</td>\n","      <td>1.207690</td>\n","      <td>1266.870634</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>NeuralNetTorch_r79_BAG_L1</td>\n","      <td>-73457.894931</td>\n","      <td>root_mean_squared_error</td>\n","      <td>1.359697</td>\n","      <td>921.746454</td>\n","      <td>1.359697</td>\n","      <td>921.746454</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>LightGBMLarge_BAG_L1</td>\n","      <td>-73620.198965</td>\n","      <td>root_mean_squared_error</td>\n","      <td>0.580270</td>\n","      <td>61.912755</td>\n","      <td>0.580270</td>\n","      <td>61.912755</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>XGBoost_BAG_L1</td>\n","      <td>-73649.378532</td>\n","      <td>root_mean_squared_error</td>\n","      <td>1.037074</td>\n","      <td>41.478207</td>\n","      <td>1.037074</td>\n","      <td>41.478207</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>ExtraTreesMSE_BAG_L2</td>\n","      <td>-74194.483143</td>\n","      <td>root_mean_squared_error</td>\n","      <td>54.974343</td>\n","      <td>5667.460711</td>\n","      <td>14.119920</td>\n","      <td>326.961244</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>LightGBM_r96_BAG_L1</td>\n","      <td>-74312.939239</td>\n","      <td>root_mean_squared_error</td>\n","      <td>0.680840</td>\n","      <td>51.478891</td>\n","      <td>0.680840</td>\n","      <td>51.478891</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>RandomForestMSE_BAG_L2</td>\n","      <td>-74471.547154</td>\n","      <td>root_mean_squared_error</td>\n","      <td>56.468437</td>\n","      <td>6520.030366</td>\n","      <td>15.614015</td>\n","      <td>1179.530900</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>CatBoost_r9_BAG_L1</td>\n","      <td>-75739.001120</td>\n","      <td>root_mean_squared_error</td>\n","      <td>0.473249</td>\n","      <td>43.728106</td>\n","      <td>0.473249</td>\n","      <td>43.728106</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>ExtraTreesMSE_BAG_L1</td>\n","      <td>-77344.072883</td>\n","      <td>root_mean_squared_error</td>\n","      <td>11.662408</td>\n","      <td>234.754137</td>\n","      <td>11.662408</td>\n","      <td>234.754137</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>RandomForestMSE_BAG_L1</td>\n","      <td>-78153.007850</td>\n","      <td>root_mean_squared_error</td>\n","      <td>12.183157</td>\n","      <td>311.900662</td>\n","      <td>12.183157</td>\n","      <td>311.900662</td>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>LightGBMLarge_BAG_L2</td>\n","      <td>-78476.878886</td>\n","      <td>root_mean_squared_error</td>\n","      <td>41.081681</td>\n","      <td>5384.204360</td>\n","      <td>0.227258</td>\n","      <td>43.704893</td>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>25</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                          model     score_val              eval_metric  \\\n","0           WeightedEnsemble_L3 -72449.253754  root_mean_squared_error   \n","1               CatBoost_BAG_L2 -72542.797184  root_mean_squared_error   \n","2           WeightedEnsemble_L2 -72600.339591  root_mean_squared_error   \n","3             LightGBMXT_BAG_L2 -72603.165649  root_mean_squared_error   \n","4               CatBoost_BAG_L1 -72812.796073  root_mean_squared_error   \n","5          CatBoost_r177_BAG_L1 -72844.316077  root_mean_squared_error   \n","6        NeuralNetFastAI_BAG_L2 -72879.922868  root_mean_squared_error   \n","7             LightGBMXT_BAG_L1 -72917.481425  root_mean_squared_error   \n","8                XGBoost_BAG_L2 -72995.075779  root_mean_squared_error   \n","9               LightGBM_BAG_L2 -73042.741028  root_mean_squared_error   \n","10        NeuralNetTorch_BAG_L2 -73042.784785  root_mean_squared_error   \n","11         LightGBM_r131_BAG_L1 -73119.010616  root_mean_squared_error   \n","12       NeuralNetFastAI_BAG_L1 -73204.822678  root_mean_squared_error   \n","13              LightGBM_BAG_L1 -73211.006130  root_mean_squared_error   \n","14  NeuralNetFastAI_r191_BAG_L1 -73257.546485  root_mean_squared_error   \n","15        NeuralNetTorch_BAG_L1 -73419.679730  root_mean_squared_error   \n","16    NeuralNetTorch_r79_BAG_L1 -73457.894931  root_mean_squared_error   \n","17         LightGBMLarge_BAG_L1 -73620.198965  root_mean_squared_error   \n","18               XGBoost_BAG_L1 -73649.378532  root_mean_squared_error   \n","19         ExtraTreesMSE_BAG_L2 -74194.483143  root_mean_squared_error   \n","20          LightGBM_r96_BAG_L1 -74312.939239  root_mean_squared_error   \n","21       RandomForestMSE_BAG_L2 -74471.547154  root_mean_squared_error   \n","22           CatBoost_r9_BAG_L1 -75739.001120  root_mean_squared_error   \n","23         ExtraTreesMSE_BAG_L1 -77344.072883  root_mean_squared_error   \n","24       RandomForestMSE_BAG_L1 -78153.007850  root_mean_squared_error   \n","25         LightGBMLarge_BAG_L2 -78476.878886  root_mean_squared_error   \n","\n","    pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  \\\n","0       72.923016  7015.015082                0.002833           0.555356   \n","1       41.507642  5417.826240                0.653220          77.326773   \n","2       35.898734  3786.125246                0.003237           0.470863   \n","3       42.533028  5430.640810                1.678606          90.141343   \n","4        1.244417   120.778952                1.244417         120.778952   \n","5        0.979802    89.376491                0.979802          89.376491   \n","6       44.351679  5998.206054                3.497256         657.706587   \n","7        0.631912    56.368863                0.631912          56.368863   \n","8       42.137048  5388.159207                1.282625          47.659740   \n","9       41.245911  5401.166497                0.391488          60.667030   \n","10      42.807665  5496.833271                1.953242         156.333805   \n","11       1.360831    87.939971                1.360831          87.939971   \n","12       3.476512  1500.940777                3.476512        1500.940777   \n","13       0.420324    52.221435                0.420324          52.221435   \n","14       3.556240   499.003131                3.556240         499.003131   \n","15       1.207690  1266.870634                1.207690        1266.870634   \n","16       1.359697   921.746454                1.359697         921.746454   \n","17       0.580270    61.912755                0.580270          61.912755   \n","18       1.037074    41.478207                1.037074          41.478207   \n","19      54.974343  5667.460711               14.119920         326.961244   \n","20       0.680840    51.478891                0.680840          51.478891   \n","21      56.468437  6520.030366               15.614015        1179.530900   \n","22       0.473249    43.728106                0.473249          43.728106   \n","23      11.662408   234.754137               11.662408         234.754137   \n","24      12.183157   311.900662               12.183157         311.900662   \n","25      41.081681  5384.204360                0.227258          43.704893   \n","\n","    stack_level  can_infer  fit_order  \n","0             3       True         26  \n","1             2       True         20  \n","2             2       True         16  \n","3             2       True         17  \n","4             1       True          4  \n","5             1       True         10  \n","6             2       True         22  \n","7             1       True          1  \n","8             2       True         23  \n","9             2       True         18  \n","10            2       True         24  \n","11            1       True         12  \n","12            1       True          6  \n","13            1       True          2  \n","14            1       True         13  \n","15            1       True          8  \n","16            1       True         11  \n","17            1       True          9  \n","18            1       True          7  \n","19            2       True         21  \n","20            1       True         15  \n","21            2       True         19  \n","22            1       True         14  \n","23            1       True          5  \n","24            1       True          3  \n","25            2       True         25  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["predictor.leaderboard()"]},{"cell_type":"markdown","id":"31e8267d","metadata":{"papermill":{"duration":0.042686,"end_time":"2024-09-10T17:57:14.824778","exception":false,"start_time":"2024-09-10T17:57:14.782092","status":"completed"},"tags":[]},"source":["**What the Leaderboard Shows:**\n","* **Model:** The name of the model that was trained. This can include various types of models such as Random Forest, Gradient Boosting, Neural Networks, etc.\n","* **Time Training:** The time taken to train the model.\n","* **Time Prediction:** The time taken to make predictions with the model.\n","* **Score Validation:** The score (e.g., RMSE) on the validation set, indicating how well the model performs on data it hasn’t seen during training.\n","* **Fit Order:** The order in which the models were trained.\n","\n","**Interpreting the Table:**\n","* **WeightedEnsemble_L2:** This is an ensemble model that combines predictions from multiple other models (e.g., LightGBM, CatBoost). It is ranked first due to its lowest RMSE on the validation set (Score_Validation).\n","* **LightGBM_BAG_L1:** A LightGBM model that was also considered. It shows slightly worse performance than the ensemble but may have taken less time to train (Training_Time)."]},{"cell_type":"markdown","id":"656a1e14","metadata":{"papermill":{"duration":0.045223,"end_time":"2024-09-10T17:57:14.913886","exception":false,"start_time":"2024-09-10T17:57:14.868663","status":"completed"},"tags":[]},"source":["# Make Predictions"]},{"cell_type":"code","execution_count":9,"id":"2ac19a3e","metadata":{"execution":{"iopub.execute_input":"2024-09-10T17:57:15.002133Z","iopub.status.busy":"2024-09-10T17:57:15.001219Z","iopub.status.idle":"2024-09-10T17:59:28.894615Z","shell.execute_reply":"2024-09-10T17:59:28.89374Z"},"papermill":{"duration":133.939965,"end_time":"2024-09-10T17:59:28.897114","exception":false,"start_time":"2024-09-10T17:57:14.957149","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n"]}],"source":["test_pred = predictor.predict(test_data)"]},{"cell_type":"markdown","id":"1a38bffb","metadata":{"papermill":{"duration":0.044362,"end_time":"2024-09-10T17:59:28.986824","exception":false,"start_time":"2024-09-10T17:59:28.942462","status":"completed"},"tags":[]},"source":["# Submission"]},{"cell_type":"code","execution_count":10,"id":"d54eef94","metadata":{"execution":{"iopub.execute_input":"2024-09-10T17:59:29.081852Z","iopub.status.busy":"2024-09-10T17:59:29.080542Z","iopub.status.idle":"2024-09-10T17:59:29.341854Z","shell.execute_reply":"2024-09-10T17:59:29.340941Z"},"papermill":{"duration":0.309845,"end_time":"2024-09-10T17:59:29.344204","exception":false,"start_time":"2024-09-10T17:59:29.034359","status":"completed"},"tags":[]},"outputs":[],"source":["submission['price'] = test_pred\n","submission.to_csv('submission.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":9057646,"sourceId":76728,"sourceType":"competition"}],"dockerImageVersionId":30761,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":11058.357178,"end_time":"2024-09-10T17:59:34.609862","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-10T14:55:16.252684","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}